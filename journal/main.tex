\documentclass{report} % [fleqn, leqno]: flush left equations, left equation numbers


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
% \usepackage{style}
\usepackage[nonatbib]{style_nips}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}
% neurips
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
% formatting
% \usepackage[fontsize=11pt, parindent=0pt]{fontsize} % or change in style.sty
\usepackage{setspace} % spacing, \setstretch{}
\usepackage{longtable} % longtables
\usepackage{titlesec}
    \titlespacing{\paragraph}{0pt}{3.25ex}{3pt}
% writing
\usepackage{amsmath} % most maths
\usepackage{graphicx} % figures 
\usepackage{subcaption} % subfigures
\usepackage{dsfont} % \mathds
\usepackage{amsthm} % \theoremstyle
\usepackage{cleveref} % clever referencing
\usepackage{amssymb} % \nsucc
\usepackage{stmaryrd} % \llbracket
    \SetSymbolFont{stmry}{bold}{U}{stmry}{m}{n} % compatibility between amsmath and stmaryrd
\usepackage{tcolorbox} % \begin{tcolorbox}[colframe=red!25,colback=blue!10,title=blue!60!black,title= More Test]\end{tcolorbox} red!25!black means 25% red, 75%black
\usepackage[colorinlistoftodos,prependcaption,textsize=small]{todonotes} % \todo
\usepackage[style=authoryear-comp, maxbibnames=8, maxcitenames=1, sorting=ydnt, uniquelist=false, citetracker=true, backend=biber]{biblatex} % backend=biber/bibtex, authoryear-comp for compact, else authoryear
    \renewcommand*{\revsdnamepunct}{} % no comma between given and family name
    \AtEveryCitekey{\ifciteseen{}{\defcounter{maxnames}{8}}} % maxnames authors for first cite, else maxcitenames
    \DeclareNameAlias{sortname}{family-given} % family then given name
    \addbibresource{biblio.bib}
% TODO: respectful citation, e.g. MSSW2018 instead of Munteanu2018, for Alexander Munteanu, Chris Schwiegelshohn, Christian Sohler, and David P. Woodruff. 2018


    % ###################
    % general commands
    \renewcommand{\epsilon}{\varepsilon}
    \renewcommand{\phi}{\varphi}
    \newcommand{\dd}{\mathrm{d}}
    \newcommand{\NN}{\mathbb{N}}
    \newcommand{\RR}{\mathbb{R}}
    \newcommand{\CC}{\mathbb{C}}
    \newcommand{\PP}[2]{\mathbb{P} \ifstrempty{#1}{}{_{#1}} \ifstrempty{#2}{}{\left[#2\right]}}
    \newcommand{\EE}[2]{\mathbb{E} \ifstrempty{#1}{}{_{#1}} \ifstrempty{#2}{}{\left[#2\right]}}
    \newcommand{\OO}{\mathcal{O}}
    \newcommand{\Var}[2]{\operatorname{\mathbb{V}ar} \ifstrempty{#1}{}{_{#1}} \ifstrempty{#2}{}{\left[#2\right]}}
    \newcommand{\Cov}[2]{\operatorname{\mathbb{C}ov} \ifstrempty{#1}{}{_{#1}} \ifstrempty{#2}{}{\left[#2\right]}}
    \newcommand{\T}{^\top}  % or ^\intercal
    \newcommand{\1}{\mathds{1}} % indicator
    \newcommand{\voones}{\boldsymbol{j}} % vector of ones
    \newcommand{\moones}{\boldsymbol{J}} % matrix of ones
    \newcommand{\intint}[2]{\llbracket #1,#2 \rrbracket} % integer interval
    \newcommand{\note}[2]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red, #1]{#2}} % \note{inline}{"note"}
    % Show fontsize: \fontname\font\ at \the\fontdimen6\font 

% ##################
% amsthm package settings
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition} % no italics in definition
\newtheorem{definition}{Definition} % [section] for reset counting in each sections 
\newtheorem{example}{Example}
% \numberwithin{equation}{chapter} % chapter already by default



% #################
% specifics commands for this report
\newcommand{\query}{f}
\newcommand{\twoquery}{g}
\newcommand{\threequery}{h}
\newcommand{\qset}{\mathcal{F}}
\newcommand{\twoqset}{\mathcal{G}}
\newcommand{\threeqset}{\mathcal{H}}
\newcommand{\loss}[1]{L \ifstrempty{#1}{}{(#1)}}
\newcommand{\estloss}[2]{\hat{L}_{#1} \ifstrempty{#2}{}{(#2)}}
\newcommand{\empdistr}[2]{\mu_{#1} \ifstrempty{#2}{}{(#2)}}
\newcommand{\meanempdistr}[1]{\mu \ifstrempty{#1}{}{(#1)}}
\newcommand{\dnu}[3]{d_{#1}  \ifstrempty{#2#3}{}{(#2,#3)}}
\newcommand{\dnude}[3]{\lvert\ifstrempty{#2#3}{\cdot}{#2-#3}\rvert}
\newcommand{\dlone}[3]{d_{L^1(#1)} \ifstrempty{#2#3}{}{(#2,#3)}}
\newcommand{\lipschitz}{\boldsymbol\ell}
\newcommand{\bound}{M}
\newcommand{\boundrate}{\mathcal{R}}
\newcommand{\covering}{\mathbf{C}}
\newcommand{\packing}{\mathbf{P}}
\newcommand{\pdim}{{d'}}
\newcommand{\opekernel}[1]{\tilde{\boldsymbol K}_{#1}}
% #################



\begin{document}
    \title{Journal}

% \maketitle
\begin{titlepage}
    \begin{center}
        \vspace*{25pt} {
        \begin{spacing}{0.05}
            \rule{400pt}{2pt}\\
            \rule{400pt}{0.75pt}\\
        \end{spacing}
        \vspace{20pt}
        \begin{spacing}{0.9}
            \fontsize{23.6pt}{26pt}\selectfont%
            \textsc{\textbf{Improving Sample Complexity of Coresets with Determinantal Point Processes}}\\%
        \end{spacing}
        \vspace{5pt}
        \begin{spacing}{0.05}
            \rule{400pt}{0.75pt}\\
            \rule{400pt}{2pt}\\
        \end{spacing}
        }
    
        \vspace*{1cm}
        \begin{large}
        \textit{By}\\%
        \end{large}
    
    
        \vspace*{4pt}
        \begin{Large}
        \textbf{Hugo Simon-Onfroy}\\%
        \end{Large}
    
    \begin{large}
        \vspace*{5cm}
        \textbf{
        A Master MVA internship report }
        \vspace*{6pt}\\
        under the supervision of
        \vspace*{20pt}\\
    
        \begin{longtable*}{ p{0.4\textwidth} p{0.5\textwidth} }
            R\'emi Bardenet & CNRS \& UniversitÃ© de Lille
            \tabularnewline Subhroshekhar Ghosh & National University of Singapore
            \tabularnewline
        \end{longtable*}
    \end{large}
    
    
        \vspace*{1cm}
    
    
        \begin{figure}[!ht]
            \centering
            \begin{subfigure}[t]{.5\linewidth}
                \centering
                \includegraphics[width=\linewidth]{pics/logoCRIStAL.png}
            \end{subfigure}
            \begin{subfigure}[t]{.4\linewidth}
                \centering
                \includegraphics[width=0.6\linewidth]{pics/logoENS_PS.jpg}
            \end{subfigure}
        \end{figure}
    \end{center}
    \end{titlepage}

    

\begin{abstract}

    The last several years have seen the rise of data sets of extraordinary scale across a variety of scientific fields. Such massive data sets bring new challenges, because of the high cost of collecting, maintaining, and interpreting them. Existing well-proven methods can become computationally impractical when dealing with millions or even billions of data points, and when they may no longer fit on a single system but instead need to be stored on clusters of servers. New algorithms are therefore required to scale to this massive data setting. 
    
    While one could concentrate on specific machine learning issues and develop many new algorithms, we concentrated in this internship on a more comprehensive strategy. We investigate coresets, which are compressed descriptions of potentially enormous data sets, and that still guarantee provable computational properties.

    Thanks to the careful selection of sample distribution, i.i.d. random sampling has proven to be one of the most effective techniques for creating coresets. However, independent samples can sometimes be extremely redundant, and one could hope that
    enforcing diversity would yield better performances. 
    
    Because proving coreset property in a general non-i.i.d. framework is difficult, we rely on determinantal point processes (DPPs), a class of point processes that naturally introduce diversity through repulsiveness. DPPs are interesting because they are a rare example of repulsive point processes with tractable theoretical properties, while still being sufficiently expressive to address a variety of problems.

    This report is an attempt to provide improved sample complexity results on coresets by the use of well chosen DPP sampling. We start with an inventory on coreset literature, then we bring to light determinantal point processes as an improvement way. Thirdly, we qualitatively justify the use of DPPs over existing sampling methods. We finish by quantitative results, conclusion, and perspectives.
    
\end{abstract}

\tableofcontents
\setstretch{1.2}



\input{parts/chapter1.tex}
\input{parts/chapter2.tex}
\input{parts/chapter3.tex}
\input{parts/chapter4.tex}






\printbibliography
%	 \bibliographystyle{chicago}
%	 \bibliography{biblio.bib}








% \begin{tcolorbox}[colback=red!10,title= Useless?]
% 	We define $\forall a,b \geq 0$ and $\forall \nu >0$
% 	\begin{equation}
% 		\dnu{\nu}{a}{b} := \frac{|a-b|}{a+b+\nu}
% 	\end{equation}
	
% 	From that definition, one can easily check
% 	\begin{proposition}
% 		\label{prop_dnu}
% 		For all $\nu>0$, the function $d_\nu$ verifies the following properties
% 		\begin{itemize}
% 			\item $d_\nu$ is a distance i.e. it verifies positivity, separation, symmetry and triangular inequality.
% 			\item $\forall a,b \geq 0,\ 0\leq \dnu{\nu}{a}{b} < \min( \frac{|a-b|}{a}, 1)$
% 			\item Moreover, if $\exists \bound\geq 0$ such that $a,b \le \bound$, then 
% 			\begin{equation*}
% 				\frac{|a-b|}{\nu + 2\bound} \le \dnu{\nu}{a}{b} \le \frac{|a-b|}{\nu}
% 			\end{equation*}
% 		\end{itemize}
% 	\end{proposition}


    
%     Using properties \cref{prop_dnu} of $\dnude{\nu}{}{}$ distance yields that
%     \begin{align*}
%         \dnude{\nu}{\empdistr{\mathcal{S}_1}{\query}}{\empdistr{\mathcal{S}_1}{\query^*}} + \dnude{\nu}{\empdistr{\mathcal{S}_2}{\query}}{\empdistr{\mathcal{S}_2}{\query^*}}
%         &\le \frac{\lvert \empdistr{\mathcal{S}_1}{\query-\query^*}\rvert}{\nu} + \frac{\lvert \empdistr{\mathcal{S}_2}{\query-\query^*}\rvert}{\nu} \\
%         &\le \frac{2}{\nu}  \dlone{\empdistr{\mathcal{S}}{}}{\query}{\query^*}
%     \end{align*}
% \end{tcolorbox}




% Note that we could also be interested in a biased cost function such as the diversified risk introduced by \cite{zhang2017dppminibatch}
% $$
% \tilde L{\query} =\frac{1}{m}\EE{x \sim \textrm{mDPP}}{\query(x)}=\frac{1}{m}\sum_{x_i \in \mathcal X} b_{i} \query\left(x_{i}\right)
% $$
% Then an unbiased estimator of $\tilde L$ is
% \begin{equation*}
% 	\hat{\tilde L}_{\textrm{mDPP}}{\query} = \frac{1}{m}\sum_{x_i\in \mathcal S} \query(x_i)
% \end{equation*}
% We can switch between $L$ and $\tilde L$, substituting $\query(x_i)$ by $\frac{b_i \query(x_i)}{m}$.




% \textbf{Theorem 3.4.} from \cite{pemantle2011rayleighconcentration}: Let $\mathbb{P}$ be a k-homogeneous probability measure on $\mathcal{B}_{n}$ satisfying the Stochastic Covering Property (SCP). Let $f$ be a 1-Lipschitz function on $\mathcal{B}_{n}$. Then
% $$
% \mathbb{P}(\lvert f-\mathbb{E} f \rvert \geq a) \leq 2 \exp \left(-\frac{a^{2}}{8 k}\right)
% $$

% Bennett inequality: Let be $(X_i)_{i\in \intint{1}{n}}$ independent and centered real-valued random variables, and $\sigma^2 = \frac{1}{n}\sum_i \Var{}{X_i}$, then for any $t>0$
% $$
% \mathbb{P}\left(\sum_{i=1}^{n} X_{i}>t\right) \leq \exp \left(-n \sigma^{2} h\left(\frac{t}{n \sigma^{2}}\right)\right)
% $$
% where $h(u)=(1+u) \log (1+u)-u$ for $u \geq 0$.

% \section{Discrete OPE}
% \begin{tcolorbox}
% 	Can we bypass the Kernel Density Estimate (KDE) in SGD paper by using discrete OPE? See \cite{gautschi2004ope}.

% \end{tcolorbox}


% \section{Holydays questions}
% \begin{itemize}
% 	\item Variance for formula for k-DPP, in \cite{zhang2017dppminibatch}.
% 	\item How $\tilde K$ eigenspaces look like ? When $n \xrightarrow[]{} \infty$ ?
% 	\begin{itemize}
% 		\item How does it compare to \cite{bardenet2020mcdpp} ?
% 		\item If $f$ is given, can I find a $K$ for which $f$ is in "good" eigenspaces (eigenvalue $\geq$ 1).
% 	\end{itemize}
% 	\item Defining discrete OPE, because discretized continuous OPE is probably not a DPP. See Gautschi Orthogonal Polynomials, 2004.
% 	\begin{itemize}
% 		\item For making links with SGD paper \cite{bardenet2021sgddpp}
% 		\item Look at the limit e.g. for Jacobi ensembles. 
% 	\end{itemize}
% \end{itemize}
% \begin{itemize}
% 	\item Take a Bernoulli and beat it with a DPP.
% 	\item Focus on metric we could have advantages on, e.g. look how variance decay with coreset size. 
% 	\item Better with direct applications e.g. on k-means or linear regression.
% \end{itemize}
% \begin{itemize}
% 	\item Strong raylegh measure positive correlation ?
% 	\item Pemantle only for kDPP?
% 	\item Hoefding/Benett like proof for $\sum_{x_i \in \mathcal S} \epsilon_i f(x_i)$ using maybe recurisve property of HKPV sampling.
% \end{itemize}

% 	\vfill




\end{document}






