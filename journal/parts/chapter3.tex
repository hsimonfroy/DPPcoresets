\chapter{Improving}
\section{Tremblay}


\section{Variance arguments}
\subsection{Three sampling cases}
\subsubsection{Multinomial case}


In the multinomial case, we have $\mathcal S \sim \mathcal M(m, q)$ i.e. $m$ i.i.d. categorical sampling with $\PP(x_i) = q(x_i)$.
Then an unbiased estimator of $L$ is
\begin{equation*}
	\estloss{\textrm{iid}}{\query} := \sum_{x_i\in \mathcal S} \frac{\query(x_i)}{m q(x_i)}
\end{equation*}
Its variance is
\begin{equation}
	\Var{\textrm{iid}}{\query} :=\frac{1}{m} \Var{}{\frac {\query(x_i)} {q(x_i)}}
	=\frac{1}{m} \sum_{x \in \mathcal{X}} \frac{\query(x)^{2}}{q(x)} -\frac{1}{m} \loss{\query}^{2} = \query\T(\frac{Q^{-1}} m - \frac{\moones} m)\query
\end{equation}
where $Q = \operatorname{diag}(q)$ and $\moones = \voones \voones \T$ the matrix full of ones. 

For any query $\query \in \queryset$, the variance is reduced to 0 by
$$
q_{\query}(x):=\frac{ \query(x)}{\loss{\query}}
$$


\subsubsection{DPP case}
In the DPP case, we have $ \mathcal S \sim \mathcal{DPP}(K)$, \,$\pi_i := K_{ii}$. Then an unbiased estimator of $L$ is
\begin{equation*}
	\estloss{\textrm{DPP}}{\query} := \sum_{x_i\in \mathcal S} \frac{\query(x_i)}{\pi_i}
\end{equation*}
Its variance can be computed using $\epsilon_i$ as the counting variable for $x_i$
\begin{align*}
	\Var{\textrm{DPP}}{\query}
:=\sum_{i, j}\EE{}{\epsilon_{i} \epsilon_{j}} \frac{\query(x_{i}) \query(x_{j})} {\pi_{i} \pi_{j}}  - \loss{\query}^{2}\\
\quad \text{with} \quad
\EE{}{\epsilon_{i} \epsilon_{j}}=
\begin{cases}
	\det(K_{\{i, j\}})=\pi_{i} \pi_{j}-K_{ij}^{2}, & \text{if } i \neq j \\
	\EE{}{\epsilon_{i}}=\pi_{i},&\text{if } i = j
\end{cases}
\end{align*}



Introducing $\Pi = \operatorname{diag}(\pi)$ and $\tilde K = \Pi^{-1}K^{\odot 2} \Pi^{-1}$, we can rewrite  

\begin{equation}
	\Var{\textrm{DPP}}{\query}=\sum_{i}\left(\frac{1}{\pi_{i}}-1\right) \query(x_{i})^{2}-\sum_{i \neq j} \frac{K_{ij}^{2}}{\pi_{i} \pi_{j}} \query(x_{i}) \query(x_{j}) =  \query\T (\Pi^{-1}  - \tilde{K}) \query 
\end{equation}

For a Bernoulli process where $\PP(x_i \in \mathcal S) = \pi_i$ independently, the DPP kernel reduces to its diagonal i.e. $K = \Pi$ then $\tilde K = I$. We denote its variance $\Var{\textrm{diag}}{}$.


\subsubsection{m-DPP case}

In the m-DPP case, we have $\mathcal S \sim \mathcal{DPP}(K) \mid |S|=m$, and the marginals $b_{i} := \mathbb{E}\left[\epsilon_{i}\right]$ have an analytic form. Then an unbiased estimator of $L$ is
\begin{equation*}
	\estloss{\textrm{mDPP}}{\query} = \sum_{x_i\in \mathcal S} \frac{\query(x_i)}{b_i}
\end{equation*}

Note that we could also be interested in a biased cost function such as the diversified risk introduced by \cite{zhang2017dppminibatch}
$$
\tilde L({\query}) =\frac{1}{m}\EE{}{x \sim \textrm{mDPP}}[\query(x)]=\frac{1}{m}\sum_{x_i \in \mathcal X} b_{i} \query\left(x_{i}\right)
$$
Then an unbiased estimator of $\tilde L$ is
\begin{equation*}
	\hat{\tilde L}_{\textrm{mDPP}}({\query}) = \frac{1}{m}\sum_{x_i\in \mathcal S} \query(x_i)
\end{equation*}
We can switch between $L$ and $\tilde L$, substituting $\query(x_i)$ by $\frac{b_i \query(x_i)}{m}$.

Returning to the estimation of $L$, we are interested in the variance of $\hat L_{\textrm{mDPP}}$ which is
\begin{equation}
	\Var{\textrm{mDPP}}{\query}=\sum_{i}\left(\frac{1}{b_i}-1\right) \query(x_i)^2
	+ \sum_{i \neq j} C_{ij}\query(x_i) \query(x_j)
\end{equation}
where $C_{ij}=\frac{\mathbb{E}\left[\left(\epsilon_{i}-b_{i}\right)\left(\epsilon_{j}-b_{j}\right)\right]}{\mathbb{E}\left[\epsilon_{i}\right] \mathbb{E}\left[\epsilon_{j}\right]}=\frac{\mathbb{E}\left[\epsilon_{i} \epsilon_{j}\right]}{b_{i} b_{j}}-1
$

Observe that if the m-DPP kernel is reduced to its diagonal ($C_{ij} = 0$), we recover $\Var{\textrm{diag}}{}$, the variance of a Bernoulli process with same marginals ($\pi_i = b_i$), though here the number of elements sampled is fixed to $m$.

In order to benefit from some variance reduction, one should want $\forall i\neq j \,,\, C_{ij}\query(x_i) \query(x_j) <0$ for a given m-DPP.
\cite{zhang2017dppminibatch} discuss that intuitively, if the m-DPP kernel rely on some similarity measure and that $f$ is smooth for it, then 2 similar points should have both negative correlation ($C_{ij}<0$) and their value have positive scalar product ($\query(x_i) \query(x_j) > 0$). Reversely, it is argued that 2 dissimilar points should have positive correlation 
										\note{}{contradiction with property of strong Rayleigh measures}
, and their value show "no tendency to align" hinting $\query(x_i) \query(x_j) < 0$. We could more conservatively consider that the induced variance change, whether positive or negative, would in either case be small, as for DPP and m-DPP, 2 dissimilar points tend toward independence.



Note that we could also be interested in a biased cost function such as the diversified risk introduced by \cite{zhang2017dppminibatch}
$$
\tilde L{\query} =\frac{1}{m}\EE{}{x \sim \textrm{mDPP}}[\query(x)]=\frac{1}{m}\sum_{x_i \in \mathcal X} b_{i} \query\left(x_{i}\right)
$$
Then an unbiased estimator of $\tilde L$ is
\begin{equation*}
	\hat{\tilde L}_{\textrm{mDPP}}{\query} = \frac{1}{m}\sum_{x_i\in \mathcal S} \query(x_i)
\end{equation*}
We can switch between $L$ and $\tilde L$, substituting $\query(x_i)$ by $\frac{b_i \query(x_i)}{m}$.
\note{}{complete}


\subsection{Variance comparison}
In order to compare processes with same marginals, we set $\Pi = mQ$. Then $\Var{\textrm{iid}}{}$, $\Var{\textrm{diag}}{}$ and $\Var{\textrm{DPP}}{}$ are quadratic forms of $\query$ associated with respective matrices
$$\begin{cases}
	\Var{\textrm{iid}}{} \equiv \Pi^{-1} - \frac{\moones}{m} \\
	\Var{\textrm{diag}}{} \equiv \Pi^{-1} - I \\
	\Var{\textrm{DPP}}{} \equiv \Pi^{-1} - \tilde K
\end{cases}$$

\subsubsection{Comparing DPP versus diag}
The DPP variance strictly beats uniformly the Bernoulli process variance if $\tilde K$ strictly dominates identity i.e. 
\begin{equation}
	\forall \query, \, \Var{\textrm{DPP}} < \Var{\textrm{diag}} \iff \tilde K \succ I
\end{equation}
But $\tilde K$ is a symmetric positive definite matrix and by Hadamard inequality $\det( \tilde K) \leq \prod_{i} \tilde K_{ii}= 1$. Therefore at least one of its eigenvalue is lower than 1, hence $\tilde K \nsucc I$.

\subsubsection{Comparing DPP versus i.i.d.}
The DPP variance strictly beats uniformly the multinomial variance if $\tilde K$ strictly dominates $\frac{\moones}{m}$ i.e. 
\begin{equation}
	\forall \query, \, \Var{\textrm{DPP}}{} < \Var{\textrm{iid}}{} \iff \tilde K \succ \frac{\moones}{m}
\end{equation}
$K$ being symmetric positive of rank $r \in \intint{0}{n}$, it exists $V \in \RR^{r \times n}$ such that $K = V\T V$, and we denote by $V_i$ its colons, for $i \in \intint{1}{n}$.

For any vector $v \in \RR^{r}$, \cite{copenhaver2013diagramvectors} define its diagram vector 
$$\tilde v :=
 \frac{1}{\sqrt{r-1}} ((v_k^{2}-v_l^{2} , \sqrt{2 r} v_k v_l ) \mid k<l)\T \in \RR^{r(r-1)}$$
concatenating all the differences of squares and products.

Then introducing $\tilde V = \begin{pmatrix}\tilde V_i \mid i\in\intint{1}{n}\end{pmatrix}
$ allows us to rewrite $\tilde K = \frac{\moones}{r} + \frac{r-1}{r} \tilde V\T \tilde V$ thus $\tilde K - \frac{\moones}{m} = (\frac{1}{r}-\frac{1}{m})\moones + \frac{m-1}{m} \tilde V\T \tilde V$. For having $\tilde K - \frac{\moones}{m}\succeq 0$, it is sufficient to have $m \geq r$. This is exactly the case for a projective DPP with rank $r = m$, because $m \leq r$ holds for every DPP. Therefore, for every multinomial sampling, we have a projective DPP which always beats it uniformly.



