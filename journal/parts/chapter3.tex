\chapter{Correlated importance sampling}
\label{chap_correlated_sampling}



We saw in \cref{chap_DPP} that DPPs are a restriction of correlated sampling that admits useful tractability properties. Moreover, DPPs still maintain expressiveness into the sub-category of negatively correlated sampling, which is the kind of processes we expect to perform better for sample complexity. The intuition is that negatively correlated sampling can eliminate redundancy in sampling sets, an independent sampling can not.

In this chapter, we present current results on coreset sampling with DPPs, and show qualitative results on variance reduction from DPPs. 



\section{A first result with DPPs}


\cite{tremblay2018dppcoreset} first introduce DPPs into the coreset problems, based on the idea of diversity sampling. Their results holds for both DPPs and $m$-DPPs. Since projection DPPs are precisely the intersection of both DPPs and $m$-DPPs, all results apply to them. For the sake of conciseness, we state here their result for $m$-DPPs and we refer to their article for the DPP case.

\begin{tcolorbox}
	\begin{theorem}[\cite{tremblay2018dppcoreset}]
		\label{thm__tremblay}
		Let $m \in \NN$, $K_m$ a $m$-DPP kernel and let sample $\mathcal{S} \sim \mathcal{DPP}(K_m)$. Assume that the query space $\qset$ is parametrized by some $\theta \in \Theta$, and that all Lipschitz constant with respect to $\theta$ of $\query_\theta \in \qset$ are bounded by some $\lipschitz := \sup_{x \in \mathcal{X}} \operatorname{Lip}\left\{\theta \mapsto \query_\theta(x)\right\}$.\\

		If the minimal sensitivity satisfies $\min_{x\in \mathcal{X}}\sigma(x) \geq 1/n$, then for all $\epsilon, \delta \in [0,1]$ 
		\begin{equation*}
            m \geq \frac{32}{\epsilon^{2}}\left(\max_{x\in \mathcal{X}}\frac{m\sigma(x)}{K_m(x,x)}\right)^2 \log \frac{4\eta}{\delta}
			\implies 
			\text{$\mathcal{S}$ is an $\epsilon$-coreset for $\qset$ w.p. $1-\delta$}
		\end{equation*}
		where $\eta$ is the minimal number of balls of radius $\frac{\epsilon \inf_{f}\loss{f}}{6 n \lipschitz}$ necessary to cover $\Theta$.
	\end{theorem}
\end{tcolorbox}
Note first that the fraction $\frac{m}{K_m(x,x)}$ appearing in the right hand side of the bound is due to the correlated importance sampling framework. This fraction does not appear in the i.i.d. framework because the numerator $m$ cancel with the marginal intensity $mq(x)$. In practice, this fraction can be bounded uniformly on $m$, because $K_m(x,x)$ would typically grow linearly with $m$.

Also note that typically $\log \eta = \OO\left(\pdim \log \frac{n}{\epsilon \inf_{f}\loss{f}}\right)$ with $\pdim = \operatorname{pdim}\qset$, and therefore depends on $n$ and $\epsilon$.


Thus, the obtained bound for DPPs of \cref{thm__tremblay} does not improve the sample complexity bound on coreset size in i.i.d. framework, from \cite{braverman2016coresetsota}. There are two reasons for this. 

\begin{itemize}
	\item First, the result crucially relies on a concentration inequality for strongly Rayleigh measures (especially DPPs) from \cite{pemantle2011rayleighconcentration}, which does not improve Hoeffding bound used in \cref{chap_intro_coresets}. 

	However, one important fact is that it doesn't rely on more advanced concentration for projective DPPs from \cite{breuer2013nevai} that involves the variance of the estimator. Since recent results from \cite{bardenet2020mcdpp}, it is known DPPs can improve variance rate, and we hope this result to be leveraged into an improved bound on coreset for fixed query.

	\item Second, the argument to generalize to all queries from \cite{tremblay2018dppcoreset} introduce a $\log \epsilon^{-1}$ term, and foremost a dependency in $n$, through $\eta$. If not tackled, this could ruin the effort finding improved bound for fixed queries. An improvement way would be to extend classical VC theory arguments in a correlated context.
\end{itemize}


Despite these mitigated results on concentrations, DPPs has already been shown to perform variance reduction, e.g. \cite{bardenet2020mcdpp}. In the following \cref{sec__variance_arguments}, we present qualitative variance reductions in favour of DPP and $m$-DPP sampling, against Bernoulli process sampling and multinomial sampling.


\section{Variance arguments}
\label{sec__variance_arguments}
We express variance formulas in four sampling cases: multinomial, DPP, Bernoulli process, and $m$-DPP. Then we compare these variances under a domination criteria.
\subsection{Four sampling cases}
\label{subsec__foursampl}
\paragraph{In the multinomial case}, we have $\mathcal S \sim \mathcal M(m, q)$. Then an unbiased estimator of $L$ is
\begin{equation*}
	\estloss{\textrm{iid}}{\query} := \sum_{x\in \mathcal S} \frac{\query(x)}{m q(x)}
\end{equation*}
and its variance is
\begin{equation*}
	\Var{\textrm{iid}}{\query} :=\frac{1}{m} \Var{}{\frac {\query(x)} {q(x)}}
	=\frac{1}{m} \sum_{x \in \mathcal{X}} \frac{\query(x)^{2}}{q(x)} -\frac{1}{m} \loss{\query}^{2} = \boldsymbol\query\T(\frac{Q^{-1}} m - \frac{\moones} m)\boldsymbol\query
\end{equation*}
where $\boldsymbol\query := (f(x))_{x\in \mathcal{X}}$, $Q := \operatorname{diag}(q)$ and $\moones := \voones \voones \T$ the matrix full of ones. 


\paragraph{In the DPP case}, we have $ \mathcal S \sim \mathcal{DPP}(K)$, and for all $x \in \mathcal{X}$, we denote its marginals $\pi_x := K_{xx}$. Then an unbiased estimator of $L$ is
\begin{equation*}
	\estloss{\textrm{DPP}}{\query} := \sum_{x\in \mathcal S} \frac{\query(x)}{\pi_x}
\end{equation*}
Its variance can be computed using $\epsilon_x$ as the counting variable for $x$
\begin{align*}
	\Var{\textrm{DPP}}{\query}
:=\sum_{x,y \in \mathcal{X}}\EE{}{\epsilon_{x} \epsilon_{y}} \frac{\query(x) \query(y)} {\pi_{x} \pi_{y}}  - \loss{\query}^{2}\\
\quad \text{with} \quad
\EE{}{\epsilon_{x} \epsilon_{y}}=
\begin{cases}
	\det K_{\{x, y\}}=\pi_{x} \pi_{y}-K_{xy}^{2}, & \text{if } x \neq y \\
	\EE{}{\epsilon_{x}}=\pi_{x},&\text{if } x = y
\end{cases}
\end{align*}



Introducing $\Pi := \operatorname{diag}(\pi)$ and $\tilde K := \Pi^{-1}K^{\odot 2} \Pi^{-1}$, we can rewrite  

\begin{equation}
	\Var{\textrm{DPP}}{\query}=\sum_{x \in \mathcal{X}}\left(\frac{1}{\pi_{x}}-1\right) \query(x)^{2}-\sum_{x \neq y} \frac{K_{xy}^{2}}{\pi_{x} \pi_{y}} \query(x) \query(y) =  \boldsymbol\query\T (\Pi^{-1}  - \tilde{K}) \boldsymbol\query 
\end{equation}

\paragraph{In the Bernoulli process case}, where for all $x \in \mathcal{X}$, $\PP{}{x \in \mathcal S} = \pi_x$ independently, we have a special case of DPP, where the kernel reduces to its diagonal, i.e. $K = \Pi$ and then $\tilde K = I$. We denote its variance $\Var{\textrm{diag}}{f} := \boldsymbol\query\T (\Pi^{-1}  - I) \boldsymbol\query $.


\paragraph{In the m-DPP case}, we have $\mathcal S \sim \mathcal{DPP}(K) \mid |S|=m$, and we denote its marginals $b_{x} := \mathbb{E}\left[\epsilon_{i}\right]$, that admit an analytic form one can find in \cite{kulesza2012_dpp_for_ml}. Then an unbiased estimator of $L$ is
\begin{equation*}
	\estloss{\textrm{mDPP}}{\query} := \sum_{x\in \mathcal S} \frac{\query(x)}{b_x}
\end{equation*}

and its variance is
\begin{equation}
	\Var{\textrm{mDPP}}{\query}:=\sum_{i}\left(\frac{1}{b_x}-1\right) \query(x)^2
	+ \sum_{x \neq y} C_{xy}\query(x) \query(y)
\end{equation}
where $C_{xy}:=\frac{\mathbb{E}\left[\left(\epsilon_{x}-b_{y}\right)\left(\epsilon_{y}-b_{y}\right)\right]}{\mathbb{E}\left[\epsilon_{i}\right] \mathbb{E}\left[\epsilon_{j}\right]}=\frac{\mathbb{E}\left[\epsilon_{x} \epsilon_{y}\right]}{b_{x} b_{y}}-1
$

Observe that if the m-DPP kernel is reduced to its diagonal ($C_{xy} = 0$), we recover $\Var{\textrm{diag}}{}$, the variance of a Bernoulli process with same marginals ($\pi_x = b_x$), though the former has fixed sample size $m$, and the latter not.

In order to benefit from some variance reduction, one should find a $m$-DPP where $\forall x\neq y \,,\, C_{xy}\query(x) \query(y) <0$.

\cite{zhang2017dppminibatch} discuss that intuitively, if the $m$-DPP kernel rely on some similarity measure and that $f$ is smooth for it, then 2 similar points should have both negative correlation ($C_{xy}<0$) and their value have positive scalar product ($\query(x) \query(y) > 0$). This provides variance reduction.

Reversely, they argued that 2 dissimilar points should have positive correlation, and their value show ``no tendency to align'' hinting $\query(x) \query(y) < 0$, and again providing variance reduction. However, properties of strong Rayleigh measures implies always $C_{xy}\leq0$ (see \cite{pemantle2011rayleighconcentration}). But we could more conservatively consider that, whether DPP or $m$-DPP, two dissimilar points tend toward independence. Thus the induced variance change, whether positive or negative depending on the sign of $\query(x) \query(y)$, would in either case be small. 



\subsection{Variance comparison}
In the following, we compare processes with the same marginals, and therefore set $\Pi = mQ$. Also, 
since $m$-DPP marginals admits analytic but complicated form, we drop the $m$-DPP case comparison. We show in \cref{subsec__foursampl} that $\Var{\textrm{iid}}{}$, $\Var{\textrm{diag}}{}$ and $\Var{\textrm{DPP}}{}$ are quadratic forms of $\boldsymbol\query$ associated with respective matrices
$$\begin{cases}
	\Var{\textrm{iid}}{} \equiv \Pi^{-1} - \frac{\moones}{m} \\
	\Var{\textrm{diag}}{} \equiv \Pi^{-1} - I \\
	\Var{\textrm{DPP}}{} \equiv \Pi^{-1} - \tilde K
\end{cases}$$

This allows to compare samplings through the Loewner ordering ($\preceq$) of the variance associated matrices. For instance, we say DPP variance strictly dominates Bernoulli process variance if it uniformly yields lower variance, which is equivalent to $\tilde K$ being strictly greater than identity. Formally 
\begin{equation*}
	\forall \query \in \RR^{\mathcal{X}}, \, \Var{\textrm{DPP}}{f} < \Var{\textrm{diag}}{f} \iff \tilde K \succ I.
\end{equation*}
Massaging some linear algebra thus gives

\begin{tcolorbox}
	\begin{proposition}[Variance comparison]\ \\
		\label{prop__var_comp}
		DPP variance dominates Bernoulli process variance on positive-valued functions
		\begin{align}
			\label{eqn__posvar}
			\forall \query \in \RR_+^{\mathcal{X}},\ \Var{\textrm{DPP}}{f} \leq \Var{\textrm{diag}}{f}.
		\end{align}
		In the general case of real-valued functions, DPP variance does not dominate Bernoulli process variance but does up to a factor three
		\begin{align}
			\label{eqn__nodom}
			&\exists \query \in \RR^{\mathcal{X}},\ \Var{\textrm{DPP}}{f} \geq \Var{\textrm{diag}}{f}\\
			\label{eqn__domtwo}
			&\forall \query \in \RR^{\mathcal{X}},\ \Var{\textrm{DPP}}{f} \leq 3\Var{\textrm{diag}}{f}.
		\end{align}
		Moreover, if the DPP is projective, then
		\begin{align}
			\label{eqn__projiid}
			\forall \query \in \RR^{\mathcal{X}},\ \Var{\textrm{DPP}}{f} \leq \Var{\textrm{i.i.d.}}{f}.
		\end{align}
	\end{proposition}
\end{tcolorbox}



\begin{proof}[Proof of:]\
	\begin{enumerate}
		\item[\cref{eqn__posvar}] Assume $\query \in \RR_+^{\mathcal{X}}$. Then $\boldsymbol\query\T (\tilde K - I)\boldsymbol\query = \sum_{x \neq y} \frac{K_{xy}^{2}}{\pi_{x} \pi_{y}} \query(x) \query(y) \geq 0$ and therefore $\Var{\textrm{DPP}}{f} \leq \Var{\textrm{diag}}{f}$.
		\item[\cref{eqn__nodom}] $\tilde K = \Pi^{-1}K^{\odot 2} \Pi^{-1}$ is a symmetric positive matrix and by Hadamard inequality $\det( \tilde K) \leq \prod_{x\in \mathcal{X}} \tilde K_{xx}= 1$. Therefore at least one of its eigenvalue is lower than 1, hence $\tilde K \nsucc I \iff \exists \query \in \RR^{\mathcal{X}},\ \Var{\textrm{DPP}}{f} \geq \Var{\textrm{diag}}{f}$.
		\item[\cref{eqn__domtwo}]
			For all $f\in \RR^{\mathcal{X}}$, let denote by $f = f_+ - f_-$ its decomposition into its positive and negative part, which both belong in $\RR_+^{\mathcal{X}}$. Then we have

		\begin{align*}
			\hspace{-1cm}\Var{\textrm{DPP}}{f} &= \Var{\textrm{DPP}}{f_+} + \Var{\textrm{DPP}}{f_-} - 2\Cov{\textrm{DPP}}{f_+,f_-} \ &\text{\footnotesize Al-Kashi}\\
			&\leq \Var{\textrm{DPP}}{f_+} + \Var{\textrm{DPP}}{f_-} + 2\sqrt[]{\Var{\textrm{DPP}}{f_+}\Var{\textrm{DPP}}{f_-}} \ &\text{\footnotesize Cauchy-Schwartz}\\
			&\leq \Var{\textrm{diag}}{f_+} + \Var{\textrm{diag}}{f_-} + 2\sqrt[]{\Var{\textrm{diag}}{f_+}\Var{\textrm{diag}}{f_-}} \ &\text{\footnotesize \cref{eqn__posvar}}\\
			&\leq 3\Var{\textrm{diag}}{f}
		\end{align*}

		where we lastly use that $\Var{\textrm{diag}}{f} = \Var{\textrm{diag}}{f_+} + \Var{\textrm{diag}}{f_-}$ since its associated matrix is diagonal.
		\item[\cref{eqn__projiid}]
		$K$ being symmetric positive of rank $r \in \intint{0}{n}$, it exists $V \in \RR^{r \times n}$ such that $K = V\T V$, and we denote by $V_i$ its colons, for $i \in \intint{1}{n}$.
	
		For any vector $v \in \RR^{r}$, \cite{copenhaver2013diagramvectors} define its diagram vector 
		$$\tilde v :=
			\frac{1}{\sqrt{r-1}} (v_k^{2}-v_l^{2} , \sqrt{2 r} v_k v_l )_{k<l}\T \in \RR^{r(r-1)}$$
		concatenating all the $\frac{r(r-1)}{2}$ differences of squares and $\frac{r(r-1)}{2}$ products.
		
		Then introduce $\tilde V = (\tilde V_i )_{i\in\intint{1}{n}}$, the matrix whose columns are diagram vectors of matrix $V$ columns. It allows us to rewrite $\tilde K = \frac{\moones}{r} + \frac{r-1}{r} \tilde V\T \tilde V$ thus $\tilde K - \frac{\moones}{m} = (\frac{1}{r}-\frac{1}{m})\moones + \frac{m-1}{m} \tilde V\T \tilde V$. Then in order to have 
		\begin{equation*}
			\tilde K - \frac{\moones}{m}\succeq 0 \iff \forall \query \in \RR^{\mathcal{X}},\ \Var{\textrm{DPP}}{f} \leq \Var{\textrm{i.i.d.}}{f}
		\end{equation*}
		it is sufficient to have DPP kernel $K$ such that $r \leq m$. On the other hand, we know its average number of samples is $\operatorname{Tr}K = \operatorname{Tr}\Pi^{-1} = m$, because we fixed its marginals. Moreover $\operatorname{Tr}K \leq r$ holds for every DPP, this implies $\operatorname{Tr}K=r$, and therefore it is a projective DPP. Put differently, for any multinomial sampling, we have a projective DPP that beats it uniformly.
	\end{enumerate}
	
\end{proof}

Note that \cref{eqn__domtwo} use the general inequality
\begin{equation*}
	\Var{}{f} \leq \Var{}{f_+} + \Var{}{f_-} + 2\sqrt[]{\Var{}{f_+}\Var{}{f_-}}
\end{equation*}
which justifies that in many cases, we can restrict ourselves to controlling variances of positive-valued functions without loss of generality.

In the case of positive valued functions, \cref{prop__var_comp} shows that for any Bernoulli process or multinomial sampling, taking any projective DPP sampling with same marginals would yield lower variance. This is a strong qualitative argument for the use of projective DPPs for the coreset problem, that we will now try to quantify.

