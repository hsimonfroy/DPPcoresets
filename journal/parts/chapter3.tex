\chapter{Correlated importance sampling}
\label{chap_correlated_sampling}



We saw in Chapter \ref{chap_DPP} that DPPs is a restriction of correlated sampling that admits useful tractability properties. Moreover, DPPs still maintain expressiveness into the sub-category of negatively correlated sampling, which is the kind of processes we expect to perform better for sample complexity. The intuition is that negatively correlated sampling can eliminate redundancy in sampling sets an independent sampling can not.

In this chapter, we present current results on coreset sampling with DPPs, and show qualitative results on variance reduction from DPPs. 



\section{A first result with DPPs}


\cite{tremblay2018dppcoreset} first introduce DPPs into the coreset problems, based 

\begin{tcolorbox}
	\begin{theorem}
		Let $(K_m)_{m\in \NN}$ be a sequence of projective DPP kernel and let sample $\mathcal{S} \sim \mathcal{DPP}(K_m)$. Assume that the query space $\qset$ is parametrized by some $\theta \in \Theta$, and that all Lipschitz constant with respect to $\theta$ of $\query_\theta \in \qset$ are bounded by some $\lipschitz>0$.\\

		If the minimal sensitivity $\min_{x\in \mathcal{X}}\sigma(x) \geq 1/n$, then for all $\epsilon, \delta \in [0,1]$ 
		\begin{equation*}
            m \geq \frac{32}{\epsilon^{2}}\left(\max_{x\in \mathcal{X}}\frac{m\sigma(x)}{K_m(x,x)}\right) \log \frac{4\eta}{\delta}
			\implies 
			\text{$\mathcal{S}$ is $1-\delta$-surely an $\epsilon$-coreset for $\qset$}
		\end{equation*}
		where $\eta$ is the minimal number of balls of radius $\frac{\epsilon \inf_{f}\loss{f}}{6 n \lipschitz}$ necessary to cover $\Theta$.
	\end{theorem}
\end{tcolorbox}
Note first that the fraction $\frac{m}{K_m(x,x)}$ appearing in the right hand side of the bound can in practice uniformly be bounded uniformly on $m$, because $K_m(x,x)$ would typically grow linearly with $m$. This fraction does not appear in the i.i.d. framework because the numerator $m$ cancel with the marginal intensity which is $mq(x)$.

Also note that typically $\log \eta = \OO\left(\pdim \log \frac{n}{\epsilon \inf_{f}\loss{f}}\right)$ with $\pdim = \operatorname{pdim}\qset$, and therefore is dependant on $n$ and $\epsilon$.


The obtained bound in $\OO(\epsilon^{-2}\log \delta^{-1})$ for DPPs does not improve the sample complexity bound for coreset in the i.i.d. framework. The reason is two kind. 

\begin{itemize}
	\item First, the concentration for fixed query is dependant of known concentration results for strongly Rayleigh measures from \cite{pemantle2011rayleighconcentration}, and especially for DPPs. 

	However, one important fact is that it doesn't rely on more advanced concentration for DPPs from \cite{breuer2013nevai} that involves the variance of the estimator. Since recent results from \cite{bardenet2020mcdpp}, it is known DPPs can improve variance rate, and we hope this result to be leveraged into an improved bound on coreset for fixed query.
	\item Second, the generalization to all queries argument that is made requires the same rate in $\OO(\epsilon^{-2}\log \delta^{-1})$. If not tackled, this could ruin the effort finding improved bound for fixed queries. An improving way would be to extend classical VC theory arguments in a correlated context.
\end{itemize}





\section{Variance arguments}
\subsection{Three sampling cases}
\subsubsection{Multinomial case}


In the multinomial case, we have $\mathcal S \sim \mathcal M(m, q)$ i.e. $m$ i.i.d. categorical sampling with $\forall x \in \mathcal{X},\ \PP{}{x} = q(x)$.
Then an unbiased estimator of $L$ is
\begin{equation*}
	\estloss{\textrm{iid}}{\query} := \sum_{x\in \mathcal S} \frac{\query(x)}{m q(x)}
\end{equation*}
Its variance is
\begin{equation}
	\Var{\textrm{iid}}{\query} :=\frac{1}{m} \Var{}{\frac {\query(x_i)} {q(x_i)}}
	=\frac{1}{m} \sum_{x \in \mathcal{X}} \frac{\query(x)^{2}}{q(x)} -\frac{1}{m} \loss{\query}^{2} = \query\T(\frac{Q^{-1}} m - \frac{\moones} m)\query
\end{equation}
where $Q = \operatorname{diag}(q)$ and $\moones = \voones \voones \T$ the matrix full of ones. 

For any query $\query \in \qset$, the variance is reduced to 0 by
$$
q_{\query}(x):=\frac{ \query(x)}{\loss{\query}}
$$


\subsubsection{DPP case}
In the DPP case, we have $ \mathcal S \sim \mathcal{DPP}(K)$, \,$\pi_i := K_{ii}$. Then an unbiased estimator of $L$ is
\begin{equation*}
	\estloss{\textrm{DPP}}{\query} := \sum_{x_i\in \mathcal S} \frac{\query(x_i)}{\pi_i}
\end{equation*}
Its variance can be computed using $\epsilon_i$ as the counting variable for $x_i$
\begin{align*}
	\Var{\textrm{DPP}}{\query}
:=\sum_{i, j}\EE{}{\epsilon_{i} \epsilon_{j}} \frac{\query(x_{i}) \query(x_{j})} {\pi_{i} \pi_{j}}  - \loss{\query}^{2}\\
\quad \text{with} \quad
\EE{}{\epsilon_{i} \epsilon_{j}}=
\begin{cases}
	\det(K_{\{i, j\}})=\pi_{i} \pi_{j}-K_{ij}^{2}, & \text{if } i \neq j \\
	\EE{}{\epsilon_{i}}=\pi_{i},&\text{if } i = j
\end{cases}
\end{align*}



Introducing $\Pi = \operatorname{diag}(\pi)$ and $\tilde K = \Pi^{-1}K^{\odot 2} \Pi^{-1}$, we can rewrite  

\begin{equation}
	\Var{\textrm{DPP}}{\query}=\sum_{i}\left(\frac{1}{\pi_{i}}-1\right) \query(x_{i})^{2}-\sum_{i \neq j} \frac{K_{ij}^{2}}{\pi_{i} \pi_{j}} \query(x_{i}) \query(x_{j}) =  \query\T (\Pi^{-1}  - \tilde{K}) \query 
\end{equation}

For a Bernoulli process where $\PP{}{x_i \in \mathcal S} = \pi_i$ independently, the DPP kernel reduces to its diagonal i.e. $K = \Pi$ then $\tilde K = I$. We denote its variance $\Var{\textrm{diag}}{}$.


\subsubsection{m-DPP case}

In the m-DPP case, we have $\mathcal S \sim \mathcal{DPP}(K) \mid |S|=m$, and the marginals $b_{i} := \mathbb{E}\left[\epsilon_{i}\right]$ have an analytic form. Then an unbiased estimator of $L$ is
\begin{equation*}
	\estloss{\textrm{mDPP}}{\query} = \sum_{x_i\in \mathcal S} \frac{\query(x_i)}{b_i}
\end{equation*}

Returning to the estimation of $L$, we are interested in the variance of $\hat L_{\textrm{mDPP}}$ which is
\begin{equation}
	\Var{\textrm{mDPP}}{\query}=\sum_{i}\left(\frac{1}{b_i}-1\right) \query(x_i)^2
	+ \sum_{i \neq j} C_{ij}\query(x_i) \query(x_j)
\end{equation}
where $C_{ij}=\frac{\mathbb{E}\left[\left(\epsilon_{i}-b_{i}\right)\left(\epsilon_{j}-b_{j}\right)\right]}{\mathbb{E}\left[\epsilon_{i}\right] \mathbb{E}\left[\epsilon_{j}\right]}=\frac{\mathbb{E}\left[\epsilon_{i} \epsilon_{j}\right]}{b_{i} b_{j}}-1
$

Observe that if the m-DPP kernel is reduced to its diagonal ($C_{ij} = 0$), we recover $\Var{\textrm{diag}}{}$, the variance of a Bernoulli process with same marginals ($\pi_i = b_i$), though here the number of elements sampled is fixed to $m$.

In order to benefit from some variance reduction, one should want $\forall i\neq j \,,\, C_{ij}\query(x_i) \query(x_j) <0$ for a given m-DPP.
\cite{zhang2017dppminibatch} discuss that intuitively, if the m-DPP kernel rely on some similarity measure and that $f$ is smooth for it, then 2 similar points should have both negative correlation ($C_{ij}<0$) and their value have positive scalar product ($\query(x_i) \query(x_j) > 0$). Reversely, it is argued that 2 dissimilar points should have positive correlation 
										\note{}{contradiction with property of strong Rayleigh measures}
, and their value show "no tendency to align" hinting $\query(x_i) \query(x_j) < 0$. We could more conservatively consider that the induced variance change, whether positive or negative, would in either case be small, as for DPP and m-DPP, 2 dissimilar points tend toward independence.


\subsection{Variance comparison}
In order to compare processes with same marginals, we set $\Pi = mQ$. Then $\Var{\textrm{iid}}{}$, $\Var{\textrm{diag}}{}$ and $\Var{\textrm{DPP}}{}$ are quadratic forms of $\query$ associated with respective matrices
$$\begin{cases}
	\Var{\textrm{iid}}{} \equiv \Pi^{-1} - \frac{\moones}{m} \\
	\Var{\textrm{diag}}{} \equiv \Pi^{-1} - I \\
	\Var{\textrm{DPP}}{} \equiv \Pi^{-1} - \tilde K
\end{cases}$$

\subsubsection{Comparing DPP versus diag}
The DPP variance strictly beats uniformly the Bernoulli process variance if $\tilde K$ strictly dominates identity i.e. 
\begin{equation}
	\forall \query, \, \Var{\textrm{DPP}}{} < \Var{\textrm{diag}}{} \iff \tilde K \succ I
\end{equation}
But $\tilde K$ is a symmetric positive definite matrix and by Hadamard inequality $\det( \tilde K) \leq \prod_{i} \tilde K_{ii}= 1$. Therefore at least one of its eigenvalue is lower than 1, hence $\tilde K \nsucc I$.

\subsubsection{Comparing DPP versus i.i.d.}
The DPP variance strictly beats uniformly the multinomial variance if $\tilde K$ strictly dominates $\frac{\moones}{m}$ i.e. 
\begin{equation}
	\forall \query, \, \Var{\textrm{DPP}}{} < \Var{\textrm{iid}}{} \iff \tilde K \succ \frac{\moones}{m}
\end{equation}
$K$ being symmetric positive of rank $r \in \intint{0}{n}$, it exists $V \in \RR^{r \times n}$ such that $K = V\T V$, and we denote by $V_i$ its colons, for $i \in \intint{1}{n}$.

For any vector $v \in \RR^{r}$, \cite{copenhaver2013diagramvectors} define its diagram vector 
$$\tilde v :=
 \frac{1}{\sqrt{r-1}} ((v_k^{2}-v_l^{2} , \sqrt{2 r} v_k v_l ) \mid k<l)\T \in \RR^{r(r-1)}$$
concatenating all the differences of squares and products.

Then introducing $\tilde V = \begin{pmatrix}\tilde V_i \mid i\in\intint{1}{n}\end{pmatrix}
$ allows us to rewrite $\tilde K = \frac{\moones}{r} + \frac{r-1}{r} \tilde V\T \tilde V$ thus $\tilde K - \frac{\moones}{m} = (\frac{1}{r}-\frac{1}{m})\moones + \frac{m-1}{m} \tilde V\T \tilde V$. In order to have $\tilde K - \frac{\moones}{m}\succeq 0$, it is sufficient to have $m \geq r$. This is exactly the case for a projective DPP with rank $r = m$, because $m \leq r$ holds for every DPP. Therefore, for every multinomial sampling, we have a projective DPP which always beats it uniformly.



