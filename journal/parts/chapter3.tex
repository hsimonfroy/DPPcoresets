\chapter{Correlated importance sampling}
\label{chap_correlated_sampling}



We saw in \cref{chap_DPP} that DPPs is a restriction of correlated sampling that admits useful tractability properties. Moreover, DPPs still maintain expressiveness into the sub-category of negatively correlated sampling, which is the kind of processes we expect to perform better for sample complexity. The intuition is that negatively correlated sampling can eliminate redundancy in sampling sets, an independent sampling can not.

In this chapter, we present current results on coreset sampling with DPPs, and show qualitative results on variance reduction from DPPs. 



\section{A first result with DPPs}


\cite{tremblay2018dppcoreset} first introduce DPPs into the coreset problems, based on the idea of diversity sampling. Their results holds for both DPPs and $m$-DPPs. Since projective DPPs are precisely the intersection of both DPPs and $m$-DPPs, all results apply to them. For the sake of conciseness, we state here their result for $m$-DPPs and we refer to their article for the DPP case. .

\begin{tcolorbox}
	\begin{theorem}[From \cite{tremblay2018dppcoreset}]
		Let $(K_m)_{m\in \NN}$ be a sequence of $m$-DPP kernel and let sample $\mathcal{S} \sim \mathcal{DPP}(K_m)$. Assume that the query space $\qset$ is parametrized by some $\theta \in \Theta$, and that all Lipschitz constant with respect to $\theta$ of $\query_\theta \in \qset$ are bounded by some $\lipschitz>0$.\\
		If the minimal sensitivity $\min_{x\in \mathcal{X}}\sigma(x) \geq 1/n$, then for all $\epsilon, \delta \in [0,1]$ 
		\begin{equation*}
            m \geq \frac{32}{\epsilon^{2}}\left(\max_{x\in \mathcal{X}}\frac{m\sigma(x)}{K_m(x,x)}\right)^2 \log \frac{4\eta}{\delta}
			\implies 
			\text{$\mathcal{S}$ is $1-\delta$-surely an $\epsilon$-coreset for $\qset$}
		\end{equation*}
		where $\eta$ is the minimal number of balls of radius $\frac{\epsilon \inf_{f}\loss{f}}{6 n \lipschitz}$ necessary to cover $\Theta$.
	\end{theorem}
\end{tcolorbox}
Note first that the fraction $\frac{m}{K_m(x,x)}$ appearing in the right hand side of the bound is due to the correlated importance sampling framework. This fraction does not appear in the i.i.d. framework because the numerator $m$ cancel with the marginal intensity $mq(x)$. In practice, this fraction can be bounded uniformly on $m$, because $K_m(x,x)$ would typically grow linearly with $m$.

Also note that typically $\log \eta = \OO\left(\pdim \log \frac{n}{\epsilon \inf_{f}\loss{f}}\right)$ with $\pdim = \operatorname{pdim}\qset$, and therefore is dependant on $n$ and $\epsilon$.


Thus, the obtained bound for DPPs does not improve the sample complexity bound for coreset in the i.i.d. framework. The reason is two kind. 

\begin{itemize}
	\item First, the concentration for fixed query is dependant of known concentration results for strongly Rayleigh measures from \cite{pemantle2011rayleighconcentration}, and especially for DPPs. 

	However, one important fact is that it doesn't rely on more advanced concentration for DPPs from \cite{breuer2013nevai} that involves the variance of the estimator. Since recent results from \cite{bardenet2020mcdpp}, it is known DPPs can improve variance rate, and we hope this result to be leveraged into an improved bound on coreset for fixed query.
	\item Second, the generalization to all queries argument that is made introduce a $\log \epsilon^{-1}$ term, and foremost a dependency in $n$, through $\eta$. If not tackled, this could ruin the effort finding improved bound for fixed queries. An improvement way would be to extend classical VC theory arguments in a correlated context.
\end{itemize}


Despite these mitigated results on concentrations, DPPs has already been shown to preform variance reduction, e.g. \cite{bardenet2020mcdpp}.
In the following \cref{sec__variance_arguments}, we present qualitative variance reductions in favour off DPP and $m$-DPP sampling, against Bernoulli process sampling and multinomial sampling.


\section{Variance arguments}
\label{sec__variance_arguments}
We express variance formulas in four sampling cases: multinomial, DPP, Bernoulli process, and $m$-DPP. Then we compare these variances under a domination criteria.
\subsection{Four sampling cases}
\paragraph{In the multinomial case}, we have $\mathcal S \sim \mathcal M(m, q)$. Then an unbiased estimator of $L$ is
\begin{equation*}
	\estloss{\textrm{iid}}{\query} := \sum_{x\in \mathcal S} \frac{\query(x)}{m q(x)}
\end{equation*}
and its variance is
\begin{equation*}
	\Var{\textrm{iid}}{\query} :=\frac{1}{m} \Var{}{\frac {\query(x)} {q(x)}}
	=\frac{1}{m} \sum_{x \in \mathcal{X}} \frac{\query(x)^{2}}{q(x)} -\frac{1}{m} \loss{\query}^{2} = \query\T(\frac{Q^{-1}} m - \frac{\moones} m)\query
\end{equation*}
where $Q = \operatorname{diag}(q)$ and $\moones = \voones \voones \T$ the matrix full of ones. 


\paragraph{In the DPP case}, we have $ \mathcal S \sim \mathcal{DPP}(K)$, and for all $x \in \mathcal{X}$, we denote its marginals $\pi_x := K_{xx}$. Then an unbiased estimator of $L$ is
\begin{equation*}
	\estloss{\textrm{DPP}}{\query} := \sum_{x\in \mathcal S} \frac{\query(x)}{\pi_x}
\end{equation*}
Its variance can be computed using $\epsilon_x$ as the counting variable for $x$
\begin{align*}
	\Var{\textrm{DPP}}{\query}
:=\sum_{x,y \in \mathcal{X}}\EE{}{\epsilon_{x} \epsilon_{y}} \frac{\query(x) \query(y)} {\pi_{x} \pi_{y}}  - \loss{\query}^{2}\\
\quad \text{with} \quad
\EE{}{\epsilon_{x} \epsilon_{y}}=
\begin{cases}
	\det K_{\{x, y\}}=\pi_{x} \pi_{y}-K_{xy}^{2}, & \text{if } x \neq y \\
	\EE{}{\epsilon_{x}}=\pi_{x},&\text{if } x = y
\end{cases}
\end{align*}



Introducing $\Pi = \operatorname{diag}(\pi)$ and $\tilde K = \Pi^{-1}K^{\odot 2} \Pi^{-1}$, we can rewrite  

\begin{equation}
	\Var{\textrm{DPP}}{\query}=\sum_{x \in \mathcal{X}}\left(\frac{1}{\pi_{x}}-1\right) \query(x)^{2}-\sum_{x \neq y} \frac{K_{xy}^{2}}{\pi_{x} \pi_{y}} \query(x) \query(y) =  \query\T (\Pi^{-1}  - \tilde{K}) \query 
\end{equation}

\paragraph{In the Bernoulli process case}, where for all $x \in \mathcal{X}$, $\PP{}{x \in \mathcal S} = \pi_x$ independently, we have a special case of DPP, where the kernel reduces to its diagonal, i.e. $K = \Pi$ and then $\tilde K = I$. We denote its variance $\Var{\textrm{diag}}{f} := \query\T (\Pi^{-1}  - I) \query $.


\paragraph{In the m-DPP case}, we have $\mathcal S \sim \mathcal{DPP}(K) \mid |S|=m$, and we denote the marginals $b_{i} := \mathbb{E}\left[\epsilon_{i}\right]$, that admit an analytic form. Then an unbiased estimator of $L$ is
\begin{equation*}
	\estloss{\textrm{mDPP}}{\query} = \sum_{x_i\in \mathcal S} \frac{\query(x_i)}{b_i}
\end{equation*}

and its variance is
\begin{equation}
	\Var{\textrm{mDPP}}{\query}=\sum_{i}\left(\frac{1}{b_i}-1\right) \query(x_i)^2
	+ \sum_{i \neq j} C_{ij}\query(x_i) \query(x_j)
\end{equation}
where $C_{ij}=\frac{\mathbb{E}\left[\left(\epsilon_{i}-b_{i}\right)\left(\epsilon_{j}-b_{j}\right)\right]}{\mathbb{E}\left[\epsilon_{i}\right] \mathbb{E}\left[\epsilon_{j}\right]}=\frac{\mathbb{E}\left[\epsilon_{i} \epsilon_{j}\right]}{b_{i} b_{j}}-1
$

Observe that if the m-DPP kernel is reduced to its diagonal ($C_{ij} = 0$), we recover $\Var{\textrm{diag}}{}$, the variance of a Bernoulli process with same marginals ($\pi_i = b_i$), though the former has fixed sample size $m$, and the latter not.

In order to benefit from some variance reduction, one should want $\forall x\neq y \,,\, C_{xy}\query(x) \query(y) <0$ for a given m-DPP.

\cite{zhang2017dppminibatch} discuss that intuitively, if the $m$-DPP kernel rely on some similarity measure and that $f$ is smooth for it, then 2 similar points should have both negative correlation ($C_{xy}<0$) and their value have positive scalar product ($\query(x) \query(y) > 0$). This provides variance reduction.

Reversely, it is argued that 2 dissimilar points should have positive correlation, and their value show "no tendency to align" hinting $\query(x) \query(y) < 0$, and again providing variance reduction. However, properties of strong Rayleigh measures implies always $C_{xy}\leq0$ (see \cite{pemantle2011rayleighconcentration}). But we could more conservatively consider that, whether DPP or $m$-DPP, 2 dissimilar points tend toward independence. Thus the induced variance change, whether positive or negative depending on the sign of $\query(x) \query(y)$, would in either case be small. 


\subsection{Variance comparison}
In order to compare processes with same marginals, we set $\Pi = mQ$. Then $\Var{\textrm{iid}}{}$, $\Var{\textrm{diag}}{}$ and $\Var{\textrm{DPP}}{}$ are quadratic forms of $\query$ associated with respective matrices
$$\begin{cases}
	\Var{\textrm{iid}}{} \equiv \Pi^{-1} - \frac{\moones}{m} \\
	\Var{\textrm{diag}}{} \equiv \Pi^{-1} - I \\
	\Var{\textrm{DPP}}{} \equiv \Pi^{-1} - \tilde K
\end{cases}$$

\subsubsection{Comparing DPP versus diag}
The DPP variance strictly beats uniformly the Bernoulli process variance if $\tilde K$ strictly dominates identity i.e. 
\begin{equation}
	\forall \query, \, \Var{\textrm{DPP}}{f} < \Var{\textrm{diag}}{f} \iff \tilde K \succ I
\end{equation}
But $\tilde K$ is a symmetric positive definite matrix and by Hadamard inequality $\det( \tilde K) \leq \prod_{i} \tilde K_{ii}= 1$. Therefore at least one of its eigenvalue is lower than 1, hence $\tilde K \nsucc I$.

\subsubsection{Comparing DPP versus i.i.d.}
\label{sec_dpp_vs_iid}
The DPP variance strictly beats uniformly the multinomial variance if $\tilde K$ strictly dominates $\frac{\moones}{m}$ i.e. 
\begin{equation}
	\forall \query, \, \Var{\textrm{DPP}}{f} < \Var{\textrm{iid}}{f} \iff \tilde K \succ \frac{\moones}{m}
\end{equation}
$K$ being symmetric positive of rank $r \in \intint{0}{n}$, it exists $V \in \RR^{r \times n}$ such that $K = V\T V$, and we denote by $V_i$ its colons, for $i \in \intint{1}{n}$.

For any vector $v \in \RR^{r}$, \cite{copenhaver2013diagramvectors} define its diagram vector 
$$\tilde v :=
 \frac{1}{\sqrt{r-1}} ((v_k^{2}-v_l^{2} , \sqrt{2 r} v_k v_l ) \mid k<l)\T \in \RR^{r(r-1)}$$
concatenating all the differences of squares and products.

Then introducing $\tilde V = \begin{pmatrix}\tilde V_i \mid i\in\intint{1}{n}\end{pmatrix}
$ allows us to rewrite $\tilde K = \frac{\moones}{r} + \frac{r-1}{r} \tilde V\T \tilde V$ thus $\tilde K - \frac{\moones}{m} = (\frac{1}{r}-\frac{1}{m})\moones + \frac{m-1}{m} \tilde V\T \tilde V$. In order to have $\tilde K - \frac{\moones}{m}\succeq 0$, it is sufficient to have $m \geq r$. This is exactly the case for a projective DPP with rank $r = m$, because $m \leq r$ holds for every DPP. Therefore, for every multinomial sampling, we have a projective DPP which always beats it uniformly.



