
\chapter{Introduction to coresets}

\section{Motivations}

% Let $\mathcal{X}=\begin{Bmatrix}
%     x_{i} \mid i\in \intint{1}{n}
% \end{Bmatrix}$ be a multiset (possibly with repetitions) of $n$ data points. Let $\qset$ be a space of parameters, or queries, and $f$ an element of $\qset$. We consider cost functions of the form
% \begin{equation*}
%     \loss(\query = \frac{1}{|\mathcal{X}|}\sum_{x \in \mathcal{X}} f(x)    
% \end{equation*}


% Let $\mathcal{S}=\begin{Bmatrix}
%     x_{i} \mid i\in \intint{1}{m}
% \end{Bmatrix}$ be a submultiset of $\mathcal{X}$. To each element $x \in \mathcal{S}$, associate a weight $\omega\left(x\right) \in \mathbb{R}^{+}$. Define the estimated cost associated to the weighted submultiset $\mathcal{S}$ as
% \begin{equation*}
%     \estloss{\mathcal{S}}{\query}=\frac{1}{|\mathcal{S}|}\sum_{x \in \mathcal{S}} \omega\left(x\right) f(x)
% \end{equation*}

A common if not the standard approach in machine learning
is to formulate learning problems as optimization problems.

Let $\mathcal{X}=\begin{Bmatrix}
x_{i} \mid i\in \intint{1}{n}
\end{Bmatrix}$ be a multiset (possibly with repetitions) of $n$ data points. Let $\qset$ be a space of query functions defined on $\mathcal{X}$, and $\query$ an element of $\qset$. Classical learning problem aims to find a solution $\query ^*$ in $\qset$ that minimizes a cost function $\loss{}$ over the given data $\mathcal{X}$. In this work, we focus on cost functions that are positive and additively decomposable, i.e. we consider cost functions of the form
\begin{equation*}
\loss{\query}:=\sum_{x \in \mathcal{X}} \query (x)
\end{equation*}
where the queries $\query \in \qset \subseteq \RR_+^{\mathcal{X}}$ are positive functions defined on $\mathcal{X}$.

A large amount of machine learning problems falls into that framework, including support vector machines, logistic regression, linear regression and k-means clustering. For example, the goal of the euclidean $k$-means clustering is to find a set of $k$ cluster centers in $\RR^d$ minimizing the quantization error
\begin{equation*}
\loss{\query} = \sum_{x \in \mathcal{X}} \min_{q \in \mathcal{C} } \lVert x - q \rVert^2_2
\end{equation*}
In this case, $\query$ is the squared distance to the nearest cluster center $q$ in a set of cluster centers $\mathcal{C}$. Formally, $\query  \in \qset = \left\{\min_{q \in \mathcal{C} } \lVert x - q \rVert^2_2 \mid \forall \mathcal{C}  \in \binom{\mathcal{X}}{k}\right\}  $, where $\binom{\mathcal{X}}{k}$ denotes ``from $\mathcal{X}$ choose $k$'', the set of all subsets of $\mathcal{X}$ of size $k$.

In many machine learning applications, the induced optimization problem can be hard to solve. Given a learning task, if an algorithm is too slow on large datasets, one can either speed up the algorithm or reduce the amount of data.
The second alternative is theoretically guaranteed by the "coresets" idea.
A coreset is a weighted subset of the original data with the assurance that, up to a controlled relative error, the task's estimated cost function on the coreset will match the cost calculated on the complete dataset for any learning parameter.

An elegant outcome of such property is the ability to execute learning algorithms only on the coreset, assuring nearly-equal performance while significantly reducing the computational cost. There are other algorithms that generate coresets, some of which are more specialized and are designed for a particular purpose (such as k-means, k-medians, logistic regression, etc.). Additionally, keep in mind that there are results for the coreset in both the streaming and offline settings. Nevertheless, we will concentrate here on the offline setting.


\section{The coreset property}

The key idea behind coresets is to approximate the original data
set $\mathcal{X}$ by a weighted set $\mathcal{S}$ which satisfies the coreset property. Such property then guarantee $1+\epsilon$-approximations.

Let $\mathcal{S}=\begin{Bmatrix}
x_{i} \mid i\in \intint{1}{m}
\end{Bmatrix}$ be a submultiset of $\mathcal{X}$. To each element $x \in \mathcal{S}$, associate a weight $\omega\left(x\right) \in \mathbb{R}^{+}$. Define the estimated cost based on the weighted submultiset $\mathcal{S}$ as
$$
\estloss{\mathcal{S}}{\query}:=\sum_{x \in \mathcal{S}} \omega\left(x\right) \query(x)
$$
\begin{definition}[Coreset]
    \label{def_coresetprop}
    Let $\epsilon \in {]}0,1{[}$ and $\query \in \qset$. We say $\mathcal{S}$ is an $\epsilon$-coreset for $\query$ if the estimated cost based on $\mathcal{S}$ is equal to the exact cost up to a relative error $\epsilon$. Formally
    \begin{equation}
        \label{eqn_querycoresetprop}
        \left|\frac{\estloss{\mathcal{S}}{\query}}{\loss{\query}}-1\right| \le \epsilon 
    \end{equation}
    We say $\mathcal{S}$ is an $\epsilon$-coreset for $\qset$ if it is a $\epsilon$-coreset for any $\query \in \qset$. Formally
    \begin{equation}
        \label{eqn_qsetcoresetprop}
        \forall \query \in \qset,\ \left|\frac{\estloss{\mathcal{S}}{\query}}{\loss{\query}}-1\right| \le \epsilon 
    \end{equation}
\end{definition}
An important consequence of the coreset property is the following

\begin{theorem}
    Let $\mathcal{S}$ be an $\epsilon$-coreset for $\qset$. Define $\query^*:=\min_{\query \in \qset}\loss{\query}$ and $\hat \query^*:=\min_{\query \in \qset}\estloss{\mathcal{S}}{\query}$. Then $\loss{\hat\query^*} $ is an $(1+3\epsilon)$-approximation of $\loss{\query^*}$, i.e.

    \begin{equation*}
        \loss{\query^*} \le {L}( \hat\query^*)\leq (1+ 3 \epsilon)\loss{\query^*}
    \end{equation*}
    \label{thm_optcoreset}
\end{theorem}
\begin{proof}
    If $\mathcal{S}$ is a $\epsilon$-coreset for $\qset$, we have from Equation \ref{eqn_qsetcoresetprop} that 
    \begin{equation*}
        \forall \query \in \qset ,\ (1-\epsilon) \loss{\query} \le \estloss{\mathcal{S}}{\query} \le(1+\epsilon) \loss{\query}
    \end{equation*}
    In particular, this is true for $\query^*$ and $\hat \query^*$ thus
    \begin{equation}
        (1-\epsilon) \loss{\query^*} \le(1-\epsilon) \loss{\hat\query^*} \le \estloss{\mathcal{S}}{\hat\query^*} \le \estloss{\mathcal{S}}{\query^*} \le(1+\epsilon) \loss{\query^*}
    \end{equation}
    and moreover
    \begin{equation*}
        \loss{\query^*} \le {L}( \hat\query^*) \le \frac{(1+\epsilon)}{(1-\epsilon) } \loss{\query^*} \leq (1+ 3 \epsilon)\loss{\query^*}
        \end{equation*}
\end{proof}
This key consequence allows to minimize on the estimated loss $\estloss{\mathcal{S}}{}$ and still guarantee a low error on the true loss, even when the size of $\mathcal{S}$ is small before $\mathcal{X}$ one. Therefore, it makes coreset very relevant in a machine learning context, and  inscribes them into a more general learning framework that is PAC learning.

\section{Coresets and PAC learning}
\subsection{PAC learning}In computational learning theory, probably approximately correct (PAC) learning is a framework for mathematical analysis of machine learning. It was proposed in 1984 in \cite{valiant1984learnable}. The first idea is that a learning problem can be formulate into an expected risk minimization. In another words, by learning, one is interested in minimizing errors over a distribution of guesses it would have to make. To do so, the learner will receives samples and must select a prediction function based on them. The PAC framework states that the learner ability can be quantified by how probable (the "probably" part) the learner have a low generalization error (the "approximately correct" part) in some sense.



In that framework, several practical issues can occur. 
\begin{itemize}
    \item The richness of the considered class of prediction function can be too small to embrace the complexity of the studied phenomena.
    \item The risk optimizer algorithm could struggle finding the minimizing function, for instance only finding local minima, or yielding high computational complexity.
    \item The sample complexity required for reaching a given level of "probable" in the approximately correctness can vary.
\end{itemize}

However, a learning problem is generally not separable into these three issues. This means their resolution is not independent and had to be tackled jointly. For instance, making more expressive a class of prediction function can make its optimization more difficult, or make the sample complexity required higher. The latter case is well known as overfitting.

\subsection{Link with coresets}

Let us see how coresets can naturally intervene into the PAC framework. Formally, let be given a probability distribution $\PP{}{}$ generating the data $\mathcal{S}$, and let be $\qset$ a family of loss functions. Minimizing the expected loss is equivalent to finding $\query^*:= \arg \min_{\query \in \qset} \loss{\query}$. Because we are only given $\mathcal{S}$ and not the full distribution, we have to approximate $\loss{\query}$ by some estimate $\estloss{\mathcal{S}}{\query}$ based on the data, and then minimizing it with $\hat{\query}^* := \arg \min_{\query\in \qset} \estloss{\mathcal{S}}{\query}$. 


In order to evaluate this scheme, we fix some $\epsilon>0$, and we want with the highest probability as possible that the relative error of $\loss{\hat\query^*}$ against $\loss{\query^*}$ is less than $\epsilon$. Put differently we want
\begin{equation*}
	\mathbb{P}\left[|\loss{\hat\query^*} - \loss{\query^*}| \ge \epsilon \loss{\query^*}\right] \leq \delta
\end{equation*}
for the smallest $\delta$ as possible.

But we know a sufficient condition to control this error, that's the coreset property! Indeed, suppose we sample a data set $\mathcal{S}$ such that $\mathcal{S}$ is an $\epsilon/3$-coreset for $\qset$ with probability at least $1-\delta$. Formally
\begin{equation*}
    \PP{}{\forall f \in \qset,\ |\frac{\estloss{\mathcal{S}}{\query}}{\loss{\query}} - 1| \leq \epsilon/3} \geq 1- \delta
\end{equation*}
Then by Theorem \ref{thm_optcoreset} we have $1-\delta$-surely, i.e. with probability at least $1-\delta$, that
\begin{equation*}
    \loss{\query^*} \leq \loss{\hat\query^*} \leq (1+ 3 \epsilon/3) \loss{\query^*} \iff
    |\loss{\hat\query^*} - \loss{\query^*}| \leq \epsilon \loss{\query^*}
\end{equation*}

We thus see that the PAC framework translates to approximating with high probability the evaluation of a function on a data subset, which is guaranteed by the coreset property. 

On another hand, the use of coresets leverage one of the three issues that occur in PAC learning, that is to reduce the number of samples required to compute an optimal prediction function, and still controlling the error. If the time complexity for an optimization algorithm to optimize on $n$ data points is $O(a_n)$, and that it takes $O(b_m)$ time to sample an $\epsilon$-coreset which is of size $m \le n$, then we have interest in building coreset as soon as $O(a_n) \geq O(b_m) + O(a_m)$.





    

\section{State-of-the-art results on coresets}
The existence of coresets is trivial, the original data set $\mathcal{X}$ itself  being a coreset if all its elements weighted by 1. The key question is the existence of small coresets where the coreset size is sublinear, if not independent, in the number of data points $n$, while at the same time having slow rate with respect to other parameters, in particular $d$ the dimension of data, $\epsilon$ the desired error, and in the case where the coreset is obtained probabilistically, $\delta$ the probability bound of not being a coreset.
\note{}{deterministic approches?}

\subsection{Importance multinomial sampling}

In the stochastic case, a well-established approach is importance multinomial sampling. Given any distribution $q$ on $\mathcal{X}$, one can sample a set $\mathcal{S} \subseteq \binom{\mathcal{X}}{m}$ from the multinomial distribution of size $m$ based on $q$, $\mathcal S \sim \mathcal M(m, q)$, i.e. $m$ i.i.d. categorical sampling with $\forall i \in \intint{1}{n},\ \PP{}{x_i} = q(x_i)$.

By the importance sampling trick, an unbiased estimator of $\loss{\query}$ is then
\begin{equation*}
	\estloss{\textrm{iid}}{\query} := \sum_{x\in \mathcal S} \frac{\query(x)}{m q(x)}
\end{equation*}
And its variance is
\begin{equation}
	\Var{\textrm{iid}}{\query} :=\frac{1}{m} \Var{}{\frac {\query(x)} {q(x)}}
	=\frac{1}{m} \sum_{x \in \mathcal{X}} \frac{\query(x)^{2}}{q(x)} -\frac{1}{m} \loss{\query}^{2} = \query\T(\frac{Q^{-1}} m - \frac{\moones} m)\query
\end{equation}
where $Q = \operatorname{diag}(q)$ and $\moones = \voones \voones \T$ the matrix full of ones. 


Now observe that for any query $\query \in \qset$, the variance is reduced to 0 by
\begin{equation*}
    q_{\query} :=\frac{ \query}{\loss{\query}} = x \mapsto \frac{ \query(x)}{\loss{\query}}
\end{equation*}
Of course, attempting to sample from $q_\query$ is quiet limited in practice. First, we would prefer not having to make our sampling depend on the query function $\query$. Second and main obstacle is that using $q_\query$ implies already knowing $\loss{\query}$, for which we are supposedly looking an approximation for via building coreset!

However, let pursue on this idea, see if we can already try to solve first limitation and see later what can be done for the second one.

\subsection{Sensitivity sampling}



Intuitively, in order to build a coreset of small size, we want to only select data points that are relevant. This means that for a given $x \in \mathcal{X}$, we want to make its probability to be sampled as small as possible, unless it plays a relevant role in the evaluation of $\loss{\query}$ for some $\query$, which translates to $\frac{\query}{\loss{\query}}$ being high.

The idea of \cite{langberg2010_universal_approximator} is thus to take for every $x$, the sampling probability $q_\query$ in the worst case $\query$ for which $x$ is relevant in the evaluation of $\loss{\query}$. Formally, they define the following notion of sensitivity.

\begin{definition}[Sensitivity]
	The sensitivity $\sigma(x)$ of a data point $x \in \mathcal{X}$ and the total sensitivity $\mathfrak S$ of $\mathcal X$ are defined as
	\begin{align*}
        \begin{cases}
            \sigma(x) = \sup _{\query \in \qset} \frac{{\query}\left(x\right)}{L(\query)} \quad \in[0,1]\\
            \mathfrak{S}=\sum_{x\in \mathcal{X}} \sigma(x)
        \end{cases}
    \end{align*}
\end{definition} 

\subsection{Main proof}
Let $s$ be an upper bound on sensitivity $\sigma$ i.e. $\forall x \in \mathcal{X}, s(x) \geq \sigma(x)$, and $S := \sum_{x\in \mathcal{X}} s(x)$. Furthermore, let be sampled  $\mathcal S \sim \mathcal M(m, s/S)$, the multinomial sampling case. Define $g_\query(x) :=  \frac{\query(x)}{s(x) L(\query)}  \, \in[0,1]$

By Hoeffding's inequality, we thus have for any $\query \in \qset$ and $\epsilon^{\prime}>0$
\begin{equation}
	\mathbb{P}\left[\left|\frac{1}{m} \sum_{x \in \mathcal{S}} g_{\query}(x) - \mathbb{E}\left[g_{\query}(x)\right]\right|>\epsilon^{\prime}\right] \leq 2 \exp \left(-2 m \epsilon^{\prime 2}\right)
\end{equation}
and by definition, $\mathbb{E}\left[g_{\query}(x)\right]=\frac{1}{S}$ and $\frac{1}{m} \sum_{x \in \mathcal{S}} g_{\query}(x)=\frac{\estloss{\textrm{iid}}{\query}}{S L(\query)}$, thus
\begin{equation*}
	\mathbb{P}\left[|\estloss{\textrm{iid}}{\query} - L(\query)|>\epsilon^{\prime} S L(\query)\right] \leq 2 \exp \left(-2 m \epsilon^{\prime 2}\right)
\end{equation*}
Hence, $\mathcal{S}$ satisfies the $\epsilon$-coreset for $\query$ property \ref{eqn_querycoresetprop} with probability at least $1-\delta$, if we choose
\begin{equation}
	m \geq \frac{S^{2}}{2 \epsilon^{2}} \log \frac{2}{\delta}
\end{equation}


\subsection{Extension to all queries}



\begin{tcolorbox}
	\begin{definition}[pseudo-dimension]
		The pseudo-dimension of a set $\threeqset$ of functions defined on $\mathcal{X}$, denoted by $\operatorname{pdim}\threeqset$, is the largest $\pdim$ such that 
	\begin{itemize}
		\item there exists $(x_{i})_{i\in \intint{1}{\pdim}} \subseteq \mathcal{X}^\pdim$, a sequence of $\pdim$ elements from $\mathcal{X}$,
		\item there exists $(t_i)_{i\in \intint{1}{\pdim}} \subseteq  \RR^\pdim$ a sequence of $\pdim$ real thresholds,
		\item such that for each $(b_i)_{i\in \intint{1}{\pdim}} \subseteq \{0,1\}^\pdim$
		\item there is an $\query \in \threeqset$ such that $\forall i \in \intint{1}{\pdim}$, we have $\query(x_i) \geq r_i \iff b_i = 1$. 
	\end{itemize}
	Put differently it always exists functions in $\threeqset$ to have values above or below some threshold for every $2^\pdim$ combinations of above/below.
\end{definition}
Pseudo-dimension can also be defined through VC-dimension. Indeed, considering the function
\begin{align*}
	\operatorname{above}_\query \colon \mathcal{X} \times \RR &\to \{0,1\}\\
	(x,r) &\mapsto \1\{f(x) \geq r\}
\end{align*}
we have
\begin{equation}
	\operatorname{pdim}\threeqset := \operatorname{VCdim}\{\operatorname{above}_\query \mid \query \in \threeqset\}
\end{equation}
\end{tcolorbox}





\note{}{developp sota}
See \textbf{Uniform guarantee for all queries} in \cite{bachem2017coresetML}. Introducing the pseudo-dimension $d'$, it gives
\begin{equation}
	m \geq \OO(\frac{S^{2}}{2 \epsilon^{2}} (d' + \log \frac{2}{\delta}))
\end{equation}

See \textbf{Theorem 5.5} of \cite{braverman2016coresetsota} for an improved bound.
\begin{equation}
	m \geq \OO(\frac{S}{2 \epsilon^{2}} (d' \log S + \log \frac{2}{\delta}))
\end{equation}

See \cite{bachem2017coresetML}.
