
\chapter{Introduction to coresets}

\section{Motivations}

% Let $\mathcal{X}=\begin{Bmatrix}
%     x_{i} \mid i\in \intint{1}{n}
% \end{Bmatrix}$ be a multiset (possibly with repetitions) of $n$ data points. Let $\mathcal{F}$ be a space of parameters, or queries, and $f$ an element of $\mathcal{F}$. We consider cost functions of the form
% \begin{equation*}
%     \EE f = \frac{1}{|\mathcal{X}|}\sum_{x \in \mathcal{X}} f(x)    
% \end{equation*}


% Let $\mathcal{S}=\begin{Bmatrix}
%     x_{i} \mid i\in \intint{1}{m}
% \end{Bmatrix}$ be a submultiset of $\mathcal{X}$. To each element $x \in \mathcal{S}$, associate a weight $\omega\left(x\right) \in \mathbb{R}^{+}$. Define the estimated cost associated to the weighted submultiset $\mathcal{S}$ as
% \begin{equation*}
%     \EE f_{\mathcal{S}}=\frac{1}{|\mathcal{S}|}\sum_{x \in \mathcal{S}} \omega\left(x\right) f(x)
% \end{equation*}

A common if not the standard approach in machine learning
is to formulate learning problems as optimization problems.

Let $\mathcal{X}=\begin{Bmatrix}
x_{i} \mid i\in \intint{1}{n}
\end{Bmatrix}$ be a multiset (possibly with repetitions) of $n$ data points. Let $\Theta$ be a space of parameters, or queries, and $\theta$ an element of $\Theta$. Given the data $\mathcal{X}$ and a space of possible solutions Q, one aims to find a solution $\theta^{\text {opt }}$ that minimizes a cost function $L$. In this work, we focus on cost functions that are additively decomposable, i.e. we consider cost functions of the form
\begin{equation*}
L(\theta):=\sum_{x \in \mathcal{X}} f_\theta(x)
\end{equation*}
for some function $f_\theta \in \{f_\theta \mid \theta \in \Theta\}$.

A large amount of machine learning problems falls into that framework, including support vector machines, logistic regression, linear regression and k-means clustering. For example, the goal of the euclidean k-means clustering is to find a set of k cluster centers in $\RR^d$ minimizing the quantization error
\begin{equation*}
L(\theta) = \sum_{x \in \mathcal{X}} \min_{q \in \theta} \lVert x - q \rVert^2_2
\end{equation*}
In this case, $\theta \in \binom{\mathcal{X}}{k}$, the set of all subsets of $\mathcal{X}$ of size $k$, and $f_\theta =  \min_{q \in \theta} \lVert x - q \rVert^2_2$

In many machine learning applications, the induced optimization problem can be hard to solve. Given a learning task, if an algorithm is too slow on large datasets, one can either speed up the algorithm or reduce the amount of data.
The second alternative is theoretically guaranteed by the "coresets" idea.
A coreset is a weighted subset of the original data with the assurance that, up to a controlled relative error, the task's estimated cost function on the coreset will match the cost calculated on the complete dataset for any learning parameter.

An elegant outcome of such property is the ability to execute learning algorithms only on the coreset, assuring nearly-equal performance while significantly reducing the computational cost. There are other algorithms that generate coresets, some of which are more specialized and are designed for a particular purpose (such as k-means, k-medians, logistic regression, etc.). Additionally, keep in mind that there are results for the coreset in both the streaming and offline settings. Nevertheless, we will concentrate here on the offline setting.


\section{The coreset property}

The key idea behind coresets is to approximate the original data
set $\mathcal{X}$ by a weighted set $\mathcal{S}$ which satisfies the coreset property. Such property then guarantee $1+\epsilon$-approximations.

Let $\mathcal{S}=\begin{Bmatrix}
x_{i} \mid i\in \intint{1}{m}
\end{Bmatrix}$ be a submultiset of $\mathcal{X}$. To each element $x \in \mathcal{S}$, associate a weight $\omega\left(x\right) \in \mathbb{R}^{+}$. Define the estimated cost associated to the weighted submultiset $\mathcal{S}$ as
$$
\hat{L}(\theta):=\sum_{x \in \mathcal{S}} \omega\left(x\right) f_\theta(x)
$$
\begin{definition}[Coreset]
    Let $\epsilon \in {]}0,1{[}$. $\mathcal{S}$ is a $\epsilon$-coreset for $L$ if, for any query $\theta$, the estimated cost is equal to the exact cost up to a relative error, i.e. for all $\theta \in \Theta$
    \begin{equation}
        \left|\frac{\hat{L}(\theta)}{L(\theta)}-1\right| \le \epsilon 
        \label{def_coresetprop}
    \end{equation}
\end{definition}
An important consequence of the coreset property is the following
\begin{theorem}
    Let be $\mathcal{S}$, a $\epsilon$-coreset for $L$. Define $\theta^{\text {opt }}:=\min_{\theta \in \Theta}L(\theta)$ and $\hat \theta^{\text {opt }}:=\min_{\theta \in \Theta}\hat L(\theta)$. Then $L( \hat \theta^{\text {opt }}) $ is an $(1+\epsilon)$-approximation of $L( \theta^{\text {opt }})$, i.e.
    \begin{equation*}
        L( {\theta}^{\text {opt }}) \le {L}( \hat{\theta}^{\text {opt }})\leq (1+ 3 \epsilon)L( \theta^{\text {opt }})
    \end{equation*}
    \label{thm_optcoreset}
\end{theorem}

\begin{proof}
    If $\mathcal{S}$ is a $\epsilon$-coreset for $L$, we have from \ref{def_coresetprop} that 
    \begin{equation}
        (1-\epsilon) L(\theta^{\text {opt }}) \le(1-\epsilon) L( \hat{\theta}^{\text {opt }}) \le \hat{L}( \hat{\theta}^{\text {opt }}) \le \hat{L}( \theta^{\text {opt }}) \le(1+\epsilon) L( \theta^{\text {opt }})
    \end{equation}
    and moreover
    \begin{equation}
        L( {\theta}^{\text {opt }}) \le {L}( \hat{\theta}^{\text {opt }}) \le \frac{(1+\epsilon)}{(1-\epsilon) } L( \theta^{\text {opt }}) \leq (1+ 3 \epsilon)L( \theta^{\text {opt }})
        \end{equation}
\end{proof}
This key property makes coreset very relevant in a machine learning context, and  inscribes them into a more general learning framework that is PAC learning.

\section{Coresets and PAC learning}
\subsection{PAC learning
}In computational learning theory, probably approximately correct (PAC) learning is a framework for mathematical analysis of machine learning. It was proposed in 1984 in \cite{valiant1984learnable}. The first idea is that a learning problem can be formulate into an expected risk minimization. In another words, by learning, one is interested in minimizing errors over a distribution of guesses it would have to make. To do so, the learner will receives samples and must select a prediction function based on them. The PAC framework states that the learner ability can be quantified by how probable (the "probably" part) the learner have a low generalization error (the "approximately correct" part) in some sense.



In that framework, several practical issues can occur. 
\begin{itemize}
    \item The richness of the considered class of prediction function can be too small to embrace the complexity of the studied phenomena.
    \item The risk optimizer algorithm could struggle finding the minimizing function, for instance only finding local minima, or yielding high computational complexity.
    \item The sample complexity required for reaching a given level of "probable" in the approximately correctness can vary.
\end{itemize}

However, a learning problem is generally not separable into these three issues. This means their resolution is not independent and had to be tackled jointly. For instance, making more expressive a class of prediction function can make its optimization more difficult, or make the sample complexity required higher. The latter case is well known as overfitting.

\subsection{Link with coresets}

Let us see how coresets can naturally intervene into the PAC framework. Formally, let be given a probability distribution $\PP$ generating the data $\mathcal{S}$, and let be $\mathcal{F}$ a family of loss functions. Minimizing the expected loss is equivalent to finding $f^*:= \arg \min_{f\in \mathcal{F}} \EE f$. Because we are only given $\mathcal{S}$ and not the full distribution, we have to approximate $\EE f$ by some estimate $\EE f_{\mathcal{S}}$ based on the data, and then minimizing it with $\hat{f}^* := \arg \min_{f\in \mathcal{F}} \EE f_{\mathcal{S}}$. 


In order to evaluate this scheme, we fix some $\epsilon>0$, and we want with the highest probability $1-\delta$ as possible
\begin{equation*}
    |\EE \hat f^* - \EE f^*| \le \epsilon
\end{equation*} 
Put differently we want
\begin{equation*}
	\mathbb{P}\left[|\EE \hat f^* - \EE f^*| \ge \epsilon\right] \leq \delta
\end{equation*}

But we know a sufficient condition to control this error, that's the coreset property! Indeed, if we have sample a data set $\mathcal{S}$ such that
\begin{equation*}
    \PPP{}{\forall f \in \mathcal{F},\ |\frac{\EE f_{\mathcal{S}}}{\EE f} - 1| \leq \epsilon/3} \geq 1- \delta
\end{equation*}
i.e. $\mathcal{S}$ is an $\epsilon/3$-coreset for $\EE$ with probability $1-\delta$, then by \ref{thm_optcoreset} we know 
\begin{equation*}
    \EE f^* \leq \EE \hat f^* \leq (1+ 3 \epsilon/3) \EE f^* \iff
    |\EE \hat f^* - \EE f^*| \leq \epsilon
\end{equation*}

We thus see that the PAC framework translates to approximating with high probability the evaluation of a function on a data subset, which is guaranteed by the coreset property. 

On another hand, the use of coresets leverage one of the three issues that occur in PAC learning, that is to reduce the number of samples required to compute an optimal prediction function, and still controlling the error. If the time complexity for an optimization algorithm to optimize on $n$ data points is $O(a_n)$, and that it takes $O(b_m)$ time to sample an $\epsilon$-coreset which is of size $m \le n$, then we have interest in building coreset as soon as $O(a_n) \geq O(b_m) + O(a_m)$.







\section{State-of-the-art results on coresets}
\begin{definition}[Sensitivity]
	The sensitivity $\sigma_i$ of a data point $x_{i}$ and the total sensitivity $\mathfrak S$ of $\mathcal X$ are
	$$
	\begin{cases}
		\sigma_{i}=\sup_{\theta \in \Theta} q_{\theta}(x_i) = \sup _{\theta \in \Theta} \frac{f_{\theta}\left(x_{i}\right)}{L(\theta)} \quad \in[0,1]\\
		\mathfrak{S}=\sum_{i=1}^{n} \sigma_{i}
	\end{cases}
	$$
\end{definition} 

\subsection{Main proof}
Let be $s$ an upper bound on sensitivity $\sigma$ i.e. $\forall i, s_i \geq \sigma_i$, and $S := \sum_{i=1}^n s_i$. Furthermore, let be sampled  $\mathcal S \sim \mathcal M(m, s/S)$, the multinomial sampling case. Define $g_\theta(x_i) := \frac{q_\theta(x_i)}{s_i} = \frac{f_{\theta(x_i)}}{s_i L(\theta)}  \, \in[0,1]$

By Hoeffding's inequality, we thus have for any $\theta \in \Theta$ and $\epsilon^{\prime}>0$
\begin{equation}
	\mathbb{P}\left[\left|\frac{1}{m} \sum_{x \in \mathcal{S}} g_{\theta}(x) - \mathbb{E}\left[g_{\theta}(x)\right]\right|>\epsilon^{\prime}\right] \leq 2 \exp \left(-2 m \epsilon^{\prime 2}\right)
\end{equation}
and by definition, $\mathbb{E}\left[g_{\theta}(x)\right]=\frac{1}{S}$ and $\frac{1}{m} \sum_{x \in \mathcal{S}} g_{\theta}(x)=\frac{\hat L_{\textrm{iid}}(\theta)}{S L(\theta)}$, thus
\begin{equation*}
	\mathbb{P}\left[|\hat L_{\textrm{iid}}(\theta) - L(\theta)|>\epsilon^{\prime} S L(\theta)\right] \leq 2 \exp \left(-2 m \epsilon^{\prime 2}\right)
\end{equation*}
Hence, $\mathcal{S}$ satisfies the $\epsilon$-coreset property \ref{def_coresetprop} for any single query $\theta \in \Theta$ with probability at least $1-\delta$, if we choose
\begin{equation}
	m \geq \frac{S^{2}}{2 \epsilon^{2}} \log \frac{2}{\delta}
\end{equation}


\subsection{Extension to all queries}
\note{}{developp sota}
See \textbf{Uniform guarantee for all queries} in \cite{bachem2017coresetML}. Introducing the pseudo-dimension $d'$, it gives
\begin{equation}
	m \geq \OO(\frac{S^{2}}{2 \epsilon^{2}} (d' + \log \frac{2}{\delta}))
\end{equation}

See \textbf{Theorem 5.5} of \cite{braverman2016coresetsota} for an improved bound.
\begin{equation}
	m \geq \OO(\frac{S}{2 \epsilon^{2}} (d' \log S + \log \frac{2}{\delta}))
\end{equation}

See \cite{bachem2017coresetML}.
