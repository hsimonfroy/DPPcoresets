

\chapter{Improving concentration with DPPs}
\label{chap__improv_conc_dpp}


\section{Quantitative results on variance}
\label{sec__quant_result_variance}
Based on the previous \cref{chap_correlated_sampling}, we are interested in quantifying the variance DPPs can yield, in order to leverage it into an improved sample complexity for coresets. We recall classical results on variance of Monte Carlo integration.

\subsection{Monte Carlo integration}
Denote $\mathcal{I}:= [-1,1]$ so that $\mathcal{I}^d$ is an hypercube of dimension $d$. Given some integrand $h \colon \mathcal{I}^d \to \RR$, we are interested in computing $\int_{\mathcal{I}^d} h(x) \dd \lambda(x)$, its integral on $\mathcal{I}^d$. The idea of Monte Carlo integration is to sample elements from $\mathcal{I}^d$ and to build an estimator $\estloss{}{\threequery}$ based on these elements.

We know from central limit theorem that as long as variance under sampling distribution exists, i.i.d. sampling Monte Carlo yields
\begin{equation}
	\Var{}{\estloss{}{\threequery}} \lesssim m^{-1}
\end{equation}
Moreover, we know from \cite{bakhvalov1959_approximate_calculation_integrals} that when the integrand $\threequery$ belongs to the Sobolev space $W^{s,2}(\mathcal{I})$, i.e. when its Fourier coefficients decay sufficiently rapidly,
\begin{equation}
	\label{eqn__lower_bound_variance_integration}
	\Var{}{\estloss{}{\threequery}} \gtrsim m^{-(1+\frac{2s}{d})}
\end{equation}
We refer to \cite{novak2014_complexity_numerical_integration} for a more recent inventory on complexity of numerical integration. 

Of course, there is plenty of room between this two variance rates. One would be tempted to get out of the i.i.d. framework, without assuming too much regularity of the integrand, and find an estimator that is as close as possible to the lower bound \cref{eqn__lower_bound_variance_integration}. We present next a recent result that falls into this description.

\newpage

\subsection{Improved variance rate with DPPs}

Assume data $\mathcal{X}$ is strictly included in the hypercube $\mathcal{X} \subset \mathcal{I}^d$. For some integrand $h$, we are interested in constructing an estimator of the mean value of $h$ on $\mathcal{X}$.

\cite{bardenet2021sgddpp} show the existence of a sequence of DPP kernels $(\opekernel{m})_{m\in\NN}$, whose induced estimator has asymptotic variance $\OO( m ^{-(1+\frac 1 d)})$. We describe here the main key steps of the kernel construction, and introduce needed notations.

\paragraph{Kernel construction.}
Assume data $\mathcal{X}$ is generated by random samplings from a distribution $\gamma$ supported in the interior of the hypercube $\mathcal{I}^d$. Then, one can define its kernel density estimation (KDE)
\begin{equation*}
	\tilde \gamma(y) := \frac{1}{n\Delta^d} \sum_{x\in \mathcal{X}} k\left(\frac{x-y}{\Delta}\right)
\end{equation*}
where $\Delta >0$, and $k$ is a kernel unrelated to any DPP kernel, chosen so that $\int k \dd \lambda = 1$. 

Let now $\omega$ be a strictly positive probability density function on $\mathcal{I}$, and  denote by $q = q$ its tensor product density, supported on $\mathcal{I}^d$. Applying Gram-Schmidt algorithm in $L^2(q \dd \lambda)$ on multivariate monomials returns a sequence of orthonormal polynomial functions $(\phi_k)_{k\in \NN}$, the multivariate orthonormal polynomials with respect to density $q$. We refer to \cite{gautschi2004ope} for a review of orthogonal polynomials.

From there, define the multivariate Orthogonal Polynomial Ensemble (OPE) kernel associated to $q$
\begin{equation}
	K_q^{(m)}(x,y) := \sum_{k=1}^m \phi_k(x) \phi_k(y) \colon \mathcal{I}^d \times \mathcal{I}^d \to \RR
\end{equation}
which is the outer product of the $m$ first multivariate orthonormal polynomials. Because of orthonormality property, OPE kernel are projective kernels, and so it induces a projective DPP.

Noticeably, we obtained an OPE kernel associated to $q$, that we can correct to obtain an OPE kernel associated to the KDE distribution $\tilde \gamma$, by defining 
\begin{equation}
	K_{q, \tilde{\gamma}}^{(m)}(x, y):=\sqrt{\frac{q(x)}{\tilde{\gamma}(x)}} K_q^{(m)}(x, y) \sqrt{\frac{q(y)}{\tilde{\gamma}(y)}}.
\end{equation}
However, this projective DPP kernel is still defined on $\mathcal{I}^d \times \mathcal{I}^d$ but we are interested in sampling elements from $\mathcal{X}$, not $\mathcal{I}^d$. A last operation we not detail allows to restrict this kernel into $\opekernel{m}$, a projective DPP kernel defined on $\mathcal{X} \times \mathcal{X}$, or equivalently, a matrix of size $n \times n$ indexed by the elements of $\mathcal{X}$.

\paragraph{Improved variance rate.}
It turns out an estimator based on $\opekernel{m}$ yields improved variance. In particular, Proposition 4. and Equation (S14) from \cite{bardenet2021sgddpp} state that

\newpage

\begin{tcolorbox}
	\begin{theorem}[\cite{bardenet2021sgddpp}]
		\label{thm_sgdpaper}
		Let $m\in \NN$ and $\mathcal{S} \sim  \mathcal{DPP}(\opekernel{m})$. Define for all $h \in \RR^{\mathcal{X}}$ the correlated importance sampling estimator
		\begin{equation}
			\estloss{\mathcal{S}}{\threequery} := \sum_{x \in \mathcal{S}} \frac{\threequery(x)}{\opekernel{m}(x,x)}.
		\end{equation}
		If  $\lipschitz_\threequery := \operatorname{Lip}\left\{\frac{m\threequery}{K^{(m)}_{q, \tilde \gamma}} \right\}$ i.e. the Lipschitz constant of $x \mapsto \frac{m\threequery(x)}{K^{(m)}_{q, \tilde \gamma}(x,x)}$ is defined, then 
		\begin{equation}
			\label{eqn__improved_var}
			\Var{}{\estloss{\mathcal{S}}{\threequery}} = \lipschitz^2_\threequery \OO( m ^{-(1+\frac 1 d)}) +\OO( n^{-1/2}).
		\end{equation}
	\end{theorem}
\end{tcolorbox}

It is worth to say this variance generalize to the continuous case. Actually, pre-existing results from \cite{bardenet2020mcdpp} treated this case, with same variance improvement. Since the same continuous arguments are invoked in both continuous and discrete case, it explains why the construction of $\opekernel{m}$ relies first on constructing continuous DPP kernels, though it is not known if that detour can be shortcut in the discrete case.

Also, note that the higher $d$ is, the less the variance gain in \cref{eqn__improved_var}. Intuitively, this is because as the dimension increase, all points become already repelled from each others, so the addition of repulsiveness is less and less effective, or put differently, there is less and less redundancy in an independent sampling that we can get rid of by a negatively correlated sampling.

% TODO
% \note{}{cf. independent sphere packing in high dimension?}
% \note{}{illustrate?}

We now introduce some notations
\begin{itemize}
	\item We denote by $\empdistr{\mathcal{S}}{}:=\frac{1}{|\mathcal{S}|} \sum_{x \in \mathcal{S}} \delta_x$ the empirical measure based on sample $\mathcal{S}  \subseteq \mathcal{X}$. 
	\item Hence for all function $\threequery$, we denote by $\empdistr{\mathcal{S}}{\threequery}:=\int_{\mathcal{X}} \threequery(x) d\empdistr{\mathcal{S}}{x} = \frac{1}{|\mathcal{S}|} \sum_{x \in \mathcal{S}} \threequery(x)$ the expectation of $\threequery$ with respect to $\empdistr{\mathcal{S}}{}$. Furthermore, given a distribution $\PP{}{}$ on $\mathcal{S}$, we denote by $\meanempdistr{\threequery}:=\EE{}{\empdistr{\mathcal{S}}{\threequery}}$, its expectation with respect to $\PP{}{}$. 
	\item Finally, the induced $L^1(\empdistr{\mathcal{S}}{})$ distance between two functions $\threequery$ and $\threequery'$ is denoted by $\dlone{\empdistr{\mathcal{S}}{}}{\threequery}{\threequery'}:=\empdistr{\mathcal{S}}{|\threequery - \threequery'|}$.
\end{itemize}  


so that we can reformulate the result from \cref{thm_sgdpaper} into the following corollary.
\begin{tcolorbox}
	\begin{corollary}
		\label{cor_sgdpaper}
		Let $m\in \NN$ and $\mathcal{S} \sim  \mathcal{DPP}(\opekernel{m})$.
		For all function $\twoquery \in \RR^{\mathcal{X}}$, we have
		\begin{equation*}
			\Var{}{\empdistr{\mathcal{S}}{\twoquery}} = \lipschitz^2_\twoquery \OO( m ^{-(1+\frac 1 d)}) +\OO( n^{-1/2})
		\end{equation*}
		where $\lipschitz_\twoquery := \operatorname{Lip}\left\{\frac{\opekernel{m}}{K^{(m)}_{q, \tilde \gamma}} \twoquery \right\}$ is the Lipschitz constant of $x \mapsto\frac{\opekernel{m}(x,x)}{K^{(m)}_{q, \tilde \gamma}(x,x)} \twoquery(x) $.
	\end{corollary}
\end{tcolorbox}

\begin{proof}
	We simply apply \cref{thm_sgdpaper} with $\threequery = \frac{\opekernel{m}g}{m}$, such that
	\begin{align*}
		\Var{}{\empdistr{\mathcal{S}}{\twoquery}} = \Var{}{\estloss{\mathcal{S}}{\threequery}} = \lipschitz^2_\threequery \OO( m ^{-(1+\frac 1 d)}) +\OO( n^{-1/2}),
	\end{align*}
	where $\lipschitz_\threequery$ is the Lipschitz constant of $x \mapsto \frac{m\threequery(x)}{K^{(m)}_{q, \tilde \gamma}(x,x)} = \frac{\opekernel{m}(x,x)}{K^{(m)}_{q, \tilde \gamma}(x,x)} \twoquery(x) $.

\end{proof}





\section{Regularity assumptions}
\label{sec__reg_assumptions}
In order to translate the improved variance rate from \cref{cor_sgdpaper} into a concentration inequality and then a sample complexity bound for coreset, several assumptions are made and are discussed.

Let $\qset \subseteq \RR^{\mathcal{X}}$ be the function space on which we want the coreset property to hold. With the notations of \cref{chap_intro_coresets}, define the following function space 
		\begin{equation*}
		\twoqset_m := \frac{m\qset}{\opekernel{m}\loss{\qset}} = \left\{x \mapsto \frac{m\query(x)}{\opekernel{m}(x,x)\loss{\query}} \mid \query \in \qset\right\} \subseteq \RR^{\mathcal{X}}.
	\end{equation*}
Note then that for any $\query \in \qset$, we can define $g := \frac{m\query}{\opekernel{m}\loss{\query}} \in \twoqset_m$ which verifies
	\begin{equation}
		\label{def_ftog}
		\frac{\estloss{\mathcal{S}}{\query}}{\loss{\query}} = \empdistr{\mathcal{S}}{g}
		\quad \text{ and }\quad 
		\meanempdistr{g} = \EE{}{\empdistr{\mathcal{S}}{g}} = \frac{\EE{}{\estloss{\mathcal{S}}{\query}}}{\loss{\query}} = 1.
	\end{equation}




In the following sections, we crucially assume the functions sets $\twoqset_m$ verify boundedness in Lipschitz constant, infinite norm, and pseudo-dimension.
Formally, we assume there exists positive reals $\lipschitz, \bound \in \RR_+$ and $\pdim \in \NN$, such that for all $m \in \NN$
\begin{enumerate}
	\item \label{assum__lip} $\forall \twoquery \in \twoqset_m,\ \operatorname{Lip}\left\{\frac{\opekernel{m}}{K^{(m)}_{q, \tilde \gamma}}\twoquery \right\} \leq \lipschitz $
	\item \label{assum__inf}$\forall \twoquery \in \twoqset_m,\ \|\twoquery\|_{\infty} \leq \bound $
	\item \label{assum__pseudodim}$\operatorname{pdim}\twoqset_m \leq \pdim$
\end{enumerate}

\paragraph{We justify these assumptions} by invoking asymptotic behaviour of sets $\twoqset_m$. With notations from previous \cref{sec__quant_result_variance}, we first have that for large $n$, the KDE estimation is close to the true density, namely $\tilde \gamma \xrightarrow[n \to +\infty]{} \gamma$. Second, next theorem describes the asymptotic behaviour of any OPE kernel, and in particular confirms that $K_q^{(m)}$ is of order $m$.
\begin{tcolorbox}
	\begin{theorem}[\cite{simon2010_totik_thm}]
		Assume $q$ is continuous. Then, for every $\epsilon>0$, we have uniformly for $x\in [-1+\epsilon,1-\epsilon]^d$
		\begin{equation}
			\frac{m}{K_q^{(m)}(x, x)} \xrightarrow[m \to +\infty]{} \frac{q(x)}{q_{\mathrm{eq}}(x)}
		\end{equation}
	\end{theorem}
	where $q_{\mathrm{eq}} = x \mapsto \prod_{k=1}^d \frac{1}{\pi \sqrt[]{1-x_k^2}}$.
\end{tcolorbox}

Finally, the restriction of $K^{(m)}_{q, \tilde \gamma}$ into $\opekernel{m}$ is such that for large $n$, we have $K^{(m)}_{q, \tilde \gamma} \simeq \opekernel{m}$. Put together, the asymptotic behaviour of sets $\twoqset_m$ is 
\begin{equation}
	\twoqset_m \to \twoqset := \frac{\qset\gamma}{\loss{\qset}q_{\mathrm{eq}}} = \left\{x \mapsto \frac{\query(x)\gamma(x)}{\loss{\query}q_{\mathrm{eq}}(x)} \mid \query \in \qset\right\} \subseteq \RR^{\mathcal{X}}.
\end{equation}


 Then, assumptions we made on $\twoqset_m$ for every $m\in \NN$ translates to assumptions on $\twoqset$. Indeed, if $\twoqset$ verifies boundedness in Lipschitz constant, infinite norm, and pseudo-dimension, we have
\begin{enumerate}
	\item $\forall \twoquery \in \twoqset_m,\ \operatorname{Lip}\left\{\frac{\opekernel{m}}{K^{(m)}_{q, \tilde \gamma}}\twoquery \right\} \lesssim \sup_{\query \in \qset}\operatorname{Lip}\left\{\frac{\query\gamma}{\loss{\query}q_{\mathrm{eq}}} \right\} =: \lipschitz$
	\item $\forall \twoquery \in \twoqset_m,\ \|\twoquery\|_{\infty} \lesssim \|\frac{\query\gamma}{\loss{\query}q_{\mathrm{eq}}}\|_{\infty} \leq \|\frac{\gamma}{q_{\mathrm{eq}}}\|_{\infty} =:  \bound$
	\item $\operatorname{pdim}\twoqset_m \lesssim \operatorname{pdim}\twoqset =: \pdim$
\end{enumerate}

Noticeably the bound $\bound$ does not depend on $\qset$, but only on the underlying distribution of data. Besides, $\twoqset$ plays a similar role as $\twoqset_s$ we introduced in \cref{eqn__g_sensitivity}. More than that, when $\qset$ is sufficiently rich, the pseudo-dimension of both $\twoqset$ and $\twoqset_s$ is expected to be driven by the one of $\qset$ in a similar fashion. Moreover, just like $\twoqset_s$, $\twoqset$ is a function space whose properties influence the sample complexity bound, as we will now show.






\section{Concentration for fixed query}
\subsection{Chebyshov bound}

For any $\epsilon>0$ and $m\in \NN$, define the Chebyshov concentration bound
\begin{equation}
	\label{def_boundratecheb}
	\boundrate_{\textrm{Cheb}}(\epsilon, m) := \frac{1}{\epsilon^2} \sup_{\twoqset \in \{\twoqset_m \mid m \in \NN\}} \sup_{\twoquery \in \twoqset} \Var{}{\empdistr{\mathcal{S}}{\twoquery}}.
\end{equation}
Remark that from \cref{cor_sgdpaper} and the assumption \ref{assum__lip} on Lipschitz constant that $\boundrate_{\textrm{Cheb}}(\epsilon, m) = \frac {1} {\epsilon^2}\left(\lipschitz^2 \OO( m ^{-(1+\frac 1 d)}) +\OO( n^{-1/2})\right)$. From that, it follows


\begin{tcolorbox}
	\begin{theorem}[Chebyshov bound for fixed query]
		\label{thm_chebfixedtheta} 
		Let $m\in \NN$ and $\mathcal{S} \sim  \mathcal{DPP}(\opekernel{m})$. 

		Then for all $\epsilon >0$ and all $\query \in \qset$,
		\begin{equation*}
			\PP{}{\dnude{\nu}{\estloss{\mathcal{S}}{\query}}{\loss{\query}}>\epsilon\loss{\query}} \leq \boundrate_{\textrm{Cheb}}(\epsilon, m)
		\end{equation*}
		
		
		Moreover, for all $\delta>0$ and for $n$ sufficiently large,
		\begin{equation*}
			m \gtrsim \left(\frac{\lipschitz^2}{\delta\epsilon^2} \right)^{\frac{1}{1+\frac 1 d}}
			\implies 
			\text{$\mathcal{S}$ is an $\epsilon$-coreset for $\query$ w.p. $1-\delta$}.
		\end{equation*}
	\end{theorem}
\end{tcolorbox}





\begin{proof}
	Let $\query \in \qset$, so that $g := \frac{m\query}{\opekernel{m}\loss{\query}} \in \twoqset_m$ verifies \cref{def_ftog}.

	Applying the Bienaym\'e-Chebyshov inequality, we obtain 
	\begin{align*}
		\PP{}{\dnude{\nu}{\estloss{\mathcal{S}}{\query}}{\loss{\query}}>\epsilon\loss{\query}}
		&= \PP{}{\dnude{\nu}{\frac{\estloss{\mathcal{S}}{\query}}{\loss{\query}}}{1}>\epsilon}\\
		&= \PP{}{\dnude{\nu}{\empdistr{\mathcal{S}}{\twoquery}}{\meanempdistr{\twoquery}}>\epsilon}\\ 
		&\leq \frac{1}{\varepsilon ^{2}}\Var{}{\empdistr{\mathcal{S}}{\twoquery}}\\
		&\leq \boundrate_{\textrm{Cheb}}(\epsilon, m)
		=\frac {1} {\epsilon^2}\left(\lipschitz^2 \OO( m ^{-(1+\frac 1 d)}) +\OO( n^{-1/2})\right)
	\end{align*}
	Hence, a sufficient condition for $\mathcal{S}$ to be an $\epsilon$-coreset for $\query$ w.p. $1-\delta$ is to have
	\begin{gather*}
		\boundrate_{\textrm{Cheb}}(\epsilon, m) \leq \delta 
		\iff
		m^{1+\frac 1 d} \gtrsim \frac{\lipschitz^2}{\delta \epsilon^2 + \OO(n^{-1/2})} = \frac {\lipschitz^2} {\delta\epsilon^2} \frac{1}{1 + \frac{1}{\delta \epsilon^2}\OO(n^{-1/2})}.
	\end{gather*} 
	For sufficiently large $n$ (potentially $n\gtrsim \delta^{-2} \epsilon^{-4}$), we can control the second factor and thus obtain the bound
	\begin{equation*}
		m \gtrsim \left(\frac{\lipschitz^2}{\delta\epsilon^2} \right)^{\frac{1}{1+\frac 1 d}}.
	\end{equation*}
\end{proof}

We obtained a sample complexity bound with improved dependency in $\epsilon$ compared to the one from \cref{thm_hoeffdingfixedquery} for the i.i.d. sampling framework. Our result is $\OO(\epsilon^{{-2}/(1+1/d)})$ whereas the latter is $\OO(\epsilon^{-2})$. As for the variance, the higher the dimension $d$ is, the less is the gain in sample complexity, because the less there is redundancy to get rid of by a negatively correlated sampling.

On the other hand, the dependency in $\delta$ is worsened compared to i.i.d. sampling. Our result is $\OO(\delta^{{-1}/(1+1/d)})$ whereas the latter is $\log \delta^{-1}$.
We propose to tackle this dependency, before generalizing the obtained bound to all queries.









\subsection{Breuer and Duits bound}
 
There actually exists a concentration bound for projective DPPs from \cite{breuer2013nevai}, that still can leverage the increased variance rate from \cite{bardenet2021sgddpp}, and that is tighter than Chebyshov bound.

\begin{tcolorbox}
	\begin{theorem}[\cite{breuer2013nevai}]
		\label{thm_breuer}
		Let $\epsilon>0$, any bounded function $\threequery$, any projective DPP kernel $K$, and let $\mathcal{S} \sim  \mathcal{DPP}(K)$.\\

		Then for any linear statistic $X_\threequery := \sum_{x\in\mathcal{S}}\threequery(x)$,
		\begin{equation*}
			\PP{}{\dnude{}{X_\threequery}{\EE{}{X_\threequery}} > \epsilon} \leq	
			\begin{cases}
				2 \exp \left(-\frac{\epsilon^2}{4 A \Var{}{X_\threequery}}\right) 
				& \text { if } \epsilon<\frac{2 A \Var{}{X_\threequery}}{3\|\threequery\|_{\infty}}\\
				2 \exp \left(-\frac{\epsilon}{6\|\threequery\|_{\infty}}\right) 
				& \text { otherwise }
			\end{cases}
		\end{equation*}	
		where $A \simeq 7819$ so does not depend on $\threequery$, $K$ or $\epsilon$.
	\end{theorem}
\end{tcolorbox}
Note that the second case in this concentration bound is tighter than the first. Indeed, 
\begin{equation*}
	\epsilon\geq\frac{2 A \Var{}{X_\threequery}}{3\|\threequery\|_{\infty}} \implies \frac{4 A \Var{}{X_\threequery}}{\epsilon^2} \leq \frac{3\|\threequery\|_{\infty}}{2 A \Var{}{X_\threequery}}\frac{4 A \Var{}{X_\threequery}}{\epsilon} = \frac{6\|\threequery\|_{\infty}}{\epsilon}
\end{equation*}
and thus we always have
\begin{equation}
	\label{eqn__remark_tighter}
	\PP{}{\dnude{}{X_\threequery}{\EE{}{X_\threequery}} > \epsilon} \leq	2 \exp \left(-\frac{\epsilon^2}{4 A \Var{}{X_\threequery}}\right).
\end{equation}	

Based on that remark, we define for any $\epsilon>0$ and $m\in \NN$
\begin{equation*}
	\boundrate_{\textrm{BD}}(\epsilon,m) := 2 \exp \left( \left(-
			4 A \boundrate_{\textrm{Cheb}}(\epsilon, m)
			\right)^{-1}
			\right)
\end{equation*}
which we call a Breuer and Duits (BD)-type bound, and that verifies what follows.





\begin{tcolorbox}
	\begin{theorem}[BD-type bound for fixed query]
		\label{thm_breuerfixedtheta}
		Let $m\in \NN$ and $\mathcal{S} \sim  \mathcal{DPP}(\opekernel{m})$. Then for all $\epsilon >0$ and all $\query \in \qset$,
		\begin{equation*}
			\PP{}{\dnude{\nu}{\estloss{\mathcal{S}}{\query}}{\loss{\query}}>\epsilon\loss{\query}} \leq \boundrate_{\textrm{BD}}(\epsilon, m).
		\end{equation*}
		
		Moreover, for all $\delta>0$ and for $n$ sufficiently large,
		\begin{equation*}
			m \gtrsim \left(\frac{\lipschitz^2}{\epsilon^2}\log  \frac{2}{\delta } \right)^{\frac{1}{1+\frac 1 d}}
			\implies 
			\text{$\mathcal{S}$ is an $\epsilon$-coreset for $\query$ w.p. $1-\delta$}.
		\end{equation*}
	\end{theorem}
\end{tcolorbox}
\begin{proof}
	Let $\query \in \qset$, so that $g := \frac{m\query}{\opekernel{m}\loss{\query}} \in \twoqset_m$ verifies \cref{def_ftog}.

	Then we apply the Breuer and Duits bound from \cref{thm_breuer} taking $\threequery = \frac{\twoquery}{m}$.
	\begin{align*}
		\PP{}{\dnude{\nu}{\estloss{\mathcal{S}}{\query}}{\loss{\query}}>\epsilon\loss{\query}}
		&= \PP{}{\dnude{\nu}{\empdistr{\mathcal{S}}{\twoquery}}{\meanempdistr{\twoquery}}>\epsilon}\\ 
		&\leq 2 \exp \left(
		\begin{cases}
			\frac{-\epsilon^2}{4 A \Var{}{\empdistr{\mathcal{S}}{\twoquery}}}
			& \text { if } \epsilon< \frac{2 A m \Var{}{\empdistr{\mathcal{S}}{\twoquery}}}{3\|\twoquery \|_{\infty}}\\
			\frac{-\epsilon m}{6\|\twoquery\|_{\infty}} 
			& \text { otherwise }
		\end{cases}
		\right)\\
		&\leq 	2 \exp \left(-\frac{\epsilon^2}{4 A \Var{}{\empdistr{\mathcal{S}}{\twoquery}}}\right)\\
		&\leq 2 \exp \left( 
			\left(-4 A \boundrate_{\textrm{Cheb}}(\epsilon, m)\right)^{-1}
		\right)
		= \boundrate_{\textrm{BD}}(\epsilon,m)
	\end{align*}
	where we firstly used the remark in \cref{eqn__remark_tighter}, and secondly the definition \cref{def_boundratecheb} of $\boundrate_{\textrm{Cheb}}$.
	
	
	Hence, a sufficient condition for $\mathcal{S}$ to be an $\epsilon$-coreset for $\query$ w.p. $1-\delta$ is to have
	\begin{gather*}
		\boundrate_{\textrm{BD}}(\epsilon,m) \leq \delta
		\iff 
		\left(\log \frac{2}{\delta }\right)^{-1} \geq
		4 A \boundrate_{\textrm{Cheb}}(\epsilon, m)
	\end{gather*}
	and we know from \cref{thm_chebfixedtheta} this implies that for $n$ sufficiently large 
	\begin{equation*}
		m \gtrsim \left(\frac{\lipschitz^2}{\epsilon^2} \log  \frac{2}{\delta } \right)^{\frac{1}{1+\frac 1 d}}.
	\end{equation*}
\end{proof}

\paragraph{Discussing the tighter case.} Seeing \cref{thm_breuer}, one could wonder why we didn't leverage the second tighter case, noticed in \cref{eqn__remark_tighter}. Applied to our context, it would lead to the bound 
\begin{equation*}
	m \geq \frac{6\bound}{\epsilon} \log\frac{2}{\delta}
\end{equation*}
which is better than the one we shown. But for the second case to apply and obtain this bound, one would require

\begin{align*}
	\epsilon 
	\geq  \frac{2 A m \Var{}{\empdistr{\mathcal{S}}{\twoquery}}}{3\|\twoquery \|_{\infty}}
	= \OO\left(\frac{\lipschitz^2}{\|\twoquery \|_{\infty} m^{1/d}}\right)\\
	\iff
	m \gtrsim \left(\frac{\lipschitz^2}{\epsilon\|\twoquery \|_{\infty}}\right)^d
	\implies
	m \gtrsim \left(\frac{\lipschitz^2}{\epsilon\bound}\right)^d.
\end{align*}
Thereby, the condition on $\epsilon$ translates to a much worse bound on $m$, ruining the interest of the second case.
		 


\paragraph{Compared to the i.i.d. framework bound} from \cref{thm_hoeffdingfixedquery}, we obtained a better bound on coreset size for fixed query. This corroborate the qualitative results we obtained on variance comparison in \cref{sec__variance_arguments}, by quantifying the effect of increased variance rate on sample complexity of coreset. 

Although the concentration bound from \cite{breuer2013nevai} pre-dates the works of \cite{tremblay2018dppcoreset}, it is only because of the improved variance rate from \cite{bardenet2021sgddpp} that it can yields improved concentration result. If one were to apply classical variance rate in $\OO(m^{-1})$ to it, it would find no improvements of the i.i.d. framework.



\section{Extension to all queries}
\label{sec_extension_all_queries}
In order to obtain an $\epsilon$-coreset for $\qset$, property \cref{eqn_querycoresetprop} must hold simultaneously for all queries $\query \in \qset$. To do so, recall the idea from \cref{chap_intro_coresets} of constructing a surrogate finite set that approximate well $\qset$, then apply union bound on it. We summarize this process into 

\begin{tcolorbox}
	\begin{theorem}[Infinite union bound]
		\label{thm_infinite_union_bound}
		Let $\threeqset \subseteq [0,\bound]^{\mathcal{X}}$ be a set of bounded functions defined on a base set $\mathcal{X}$, with $d'_{\threeqset} := \operatorname{pdim}\threeqset$ its pseudo-dimension (see \cref{def__pseudodim}). Let moreover $m\in \NN$ and $\PP{}{}$ a distribution supported on $\mathcal{X}^{m}$.\\

		Assume there exists a bounding function $\boundrate$ such that for all $\epsilon >0$, function $\threequery \in \threeqset$, integer $m \in \NN$, and let $\mathcal{S} \sim \PP{}{}$, we have the bound
		\begin{equation}
			\PP{}{\dnude{\nu}{\empdistr{\mathcal{S}}{\threequery}}{\meanempdistr{\threequery}} > \epsilon} \leq \boundrate(\epsilon, m).
		\end{equation}

		Then for all $\epsilon \in ]0,M]$ and all $m \in \NN$ such that $\boundrate(\epsilon/2, m) \leq 1/2$, it holds
		\begin{equation}
			\PP{}{\exists \threequery \in \threeqset,\ \dnude{\nu}{\empdistr{\mathcal{S}}{\threequery}}{\meanempdistr{\threequery}} > \epsilon} \leq 
			8\left(\frac{8e\bound}{\epsilon}\right)^{2d'_{\threeqset}} \boundrate(\epsilon/16, m).
		\end{equation}
	\end{theorem}
\end{tcolorbox}

Contrary to the classical union bound, this theorem allows to convert any reasonable uniform bound $\mathcal{R}$ into a similar bound over an infinite set. We delay its proof of in \cref{sec__proof_thm_infinite_union_bound}. Applied to the BD-type bound we obtained for fixed queries, it provides our main result on sample complexity of coreset.

\begin{tcolorbox}
	\begin{theorem}[BD-type bound for all queries]
		\label{thm_breuerallqueries}
		Let $m\in \NN$ and $\mathcal{S} \sim  \mathcal{DPP}(\opekernel{m})$. 

		Then for all $\epsilon \in ]0,\bound]$
		\begin{equation*}
			\PP{}{\exists \query \in \qset,\ \dnude{\nu}{\estloss{\mathcal{S}}{\query}}{\loss{\query}}>\epsilon\loss{\query}} 
			\leq 
			8\left(\frac{8e\bound}{\epsilon}\right)^{2\pdim}   \boundrate_{\textrm{BD}}(\epsilon/16, m)
		\end{equation*}
		
		Moreover, for all $\delta >0$ and for $n$ sufficiently large
		\begin{equation*}
			m \gtrsim \left(\frac{\lipschitz^2}{\epsilon^2} \left( \pdim \log \frac{\bound}{\epsilon} + \log \frac{1}{\delta }  \right) \right)^{\frac{1}{1+\frac 1 d}} 
			\implies 
			\text{$\mathcal{S}$ is an $\epsilon$-coreset for $\qset$ w.p. $1-\delta$}.
		\end{equation*}
	\end{theorem}
\end{tcolorbox}


\begin{proof}
	Let $\query \in \qset$, so that $g := \frac{m\query}{\opekernel{m}\loss{\query}} \in \twoqset_m$ verifies \cref{def_ftog}.

	We know from \cref{thm_breuerfixedtheta} that for all $\epsilon \in ]0,\bound]$ and $m \in \NN$ we have
	\begin{equation*}
		\PP{}{\dnude{\nu}{\empdistr{\mathcal{S}}{\twoquery}}{\meanempdistr{\twoquery}} > \epsilon}  \leq \boundrate_{\textrm{BD}}(\epsilon, m).
	\end{equation*}
	
	Moreover, the assumption \ref{assum__inf} implies $\twoqset_m \in [0,M]^{\mathcal{X}}$. The hypotheses of \cref{thm_infinite_union_bound} are thus satisfied, and we can apply it taking $\threeqset=\twoqset_m$, which yields that for all $m \in \NN$ such that $\boundrate_{\textrm{BD}}(\epsilon/2, m) \leq 1/2$, it holds
	\begin{align*}
		\PP{}{\exists \query \in \qset,\ \dnude{\nu}{\estloss{\mathcal{S}}{\query}}{\loss{\query}}>\epsilon\loss{\query}} 
		= \PP{}{\exists \twoquery \in \twoqset_m,\ \dnude{\nu}{\empdistr{\mathcal{S}}{\twoquery}}{\meanempdistr{\twoquery}} >  \epsilon}\\
		\leq  8\left(\frac{8e\bound}{\epsilon}\right)^{2d'_{\twoqset_m}}  \boundrate_{\textrm{BD}}(\epsilon/16, m)
		\leq  8\left(\frac{8e\bound}{\epsilon}\right)^{2\pdim} \boundrate_{\textrm{BD}}(\epsilon/16, m)
	\end{align*}
	where we used the assumption \ref{assum__pseudodim} that for all $m \in \NN$, $\operatorname{pdim}\twoqset_m \leq \pdim$.
	
	Hence, a sufficient condition for $\mathcal{S}$ to be an $\epsilon$-coreset for $\qset$ w.p. $1-\delta$ is to have
	\begin{gather*}
		\boundrate_{\textrm{BD}}(\epsilon/16, m) \leq \frac{\delta}{8} \left(\frac{8e\bound}{\epsilon}\right)^{-2\pdim}
		\iff
		m \gtrsim \left(\frac{256\lipschitz^2}{\epsilon^2} \log \left( \frac{16}{\delta }\left(\frac{8e\bound}{\epsilon}\right)^{2\pdim}\right) \right)^{\frac{1}{1+\frac 1 d}}\\
		\iff m \gtrsim \left(\frac{\lipschitz^2}{\epsilon^2} \left(\log \frac{1}{\delta } + \pdim \log \frac{\bound}{\epsilon} \right)\right)^{\frac{1}{1+\frac 1 d}}.
	\end{gather*}
	This rate is conditioned to the fact that $m$ is such that $\boundrate_{\textrm{BD}}(\epsilon/2, m) \leq 1/2$. But we know it holds as soon as $m \gtrsim \left(\frac{4\lipschitz^2}{\epsilon^2} \log 4 \right)^{\frac{1}{1+\frac 1 d}}$, which is trivially implied by the obtained bound.

\end{proof}



Our results is to be compared to \cref{eqn__sota_coreset}
\begin{equation*}
	m \gtrsim \frac{S}{\epsilon^{2}} (\operatorname{pdim}\twoqset_s \log S + \log \frac{2}{\delta}).
\end{equation*}

Discussing assumptions in \cref{sec__reg_assumptions}, we remarked that for sufficiently rich query space $\qset$, $\operatorname{pdim}\twoqset_s$ can be quiet comparable to $\pdim$. Also was noticed that $\bound$ does not need to depend on $\qset$. With this in mind, our bound is quiet comparable to the state-of-the-art, and improve it by a $(1+1/d)$-root. However, our result requires a supplementary condition on Lipschitz constant. Perspectives of improvements are discussed in \cref{sec__discussion_perspectives} 



\section{Proof of \cref{thm_infinite_union_bound}}
\label{sec__proof_thm_infinite_union_bound}


We follow a similar proof scheme as in section 9.4 of \cite{haussler1992decisiontheoricgeneralizationofPACmodel}. We specifically revisit Lemmas 12. and 13., getting rid of independency hypothesis, and making intermediary results more flexible to further improvements. Importantly, the proof is still in progress, the remaining work being reduced to \cref{lem_infi_union_bound}.

We start by a symmetrisation argument that relates a concentration statement between a sampling and its expectancy, with a concentration statement between two i.i.d. samplings. 

\begin{tcolorbox}
	\begin{lemma}[Symmetrisation]
		\label{lem_symm}
		Assume the hypothesis of \cref{thm_infinite_union_bound} about bounding function $\boundrate$. Let furthermore $\epsilon>0$, $m\in \NN$, and $\mathcal{S}_1, \mathcal{S}_2 \overset{i.i.d.}{\sim} \PP{}{}$, two sequences of size $m$ independently sampled from the same distribution supported on $\mathcal{X}^m$.\\
		  
		Then for all $m \in \NN$ such that $\boundrate(\epsilon/2, m) \leq 1/2$
		\begin{equation*}
			\PP{}{ \exists \threequery \in \threeqset,\ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\meanempdistr{\threequery}} > \epsilon} 
			\leq 2\PP{}{\exists \threequery \in \threeqset,\ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} > \epsilon/2 }
		\end{equation*}
	\end{lemma}
\end{tcolorbox}


\begin{proof}
	Let $\epsilon>0$ and take $m \in \NN$ such that $\boundrate(\epsilon/2, m) \leq 1/2$. Then let $\mathcal{S}_1$ be sampled such that $\exists \threequery \in \threeqset,\ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\meanempdistr{\threequery}} > \epsilon$. This obviously happens with probability $\PP{}{\exists \threequery \in \threeqset, \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\meanempdistr{\threequery}} > \epsilon }$.

	For such an $f$, we then independently sample $\mathcal{S}_2$ such that $\dnude{\nu}{\empdistr{\mathcal{S}_2}{\threequery}}{\meanempdistr{\threequery}} \leq \epsilon/2$. Because $\boundrate(\epsilon, m) \leq 1/2$, we know this happens with probability greater than $1-1/2 = 1/2$, and we thus have
	\begin{align*}
		&\frac 1 2\PP{}{ \exists \threequery \in \threeqset,\ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\meanempdistr{\threequery}} > \epsilon}
		\\
		\leq\ &\PP{}{\exists \threequery \in \threeqset,\ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\meanempdistr{\threequery}} > \epsilon \wedge \dnude{\nu}{\empdistr{\mathcal{S}_2}{\threequery}}{\meanempdistr{\threequery}} \leq \epsilon/2 } \\
		\leq\ &\PP{}{\exists \threequery \in \threeqset,\ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} > \epsilon/2 }
	\end{align*}
	where we used the triangular inequality 
	\begin{equation*}
		\dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\meanempdistr{\threequery}} - \dnude{\nu}{\empdistr{\mathcal{S}_2}{\threequery}}{\meanempdistr{\threequery}} \leq   \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}}.
	\end{equation*} 
\end{proof}





We then introduce the notion of $\epsilon$-packing and $\epsilon$-covering, on which rely the construction of a surrogate query set that approximate $\qset$ to an $\epsilon$ granularity.

\begin{tcolorbox}
    \begin{definition}[Separated set and packing number]
        Let $(\threeqset, d)$ be a metric space. 
        \begin{itemize}
            \item For any $\epsilon> 0$, a subset $\threeqset' \subseteq \threeqset$ is said to be $\epsilon$-separated if for all distinct $\threequery'_1, \threequery'_2 \in \threeqset'$, $d(\threequery'_1, \threequery'_2) > \epsilon$.
            \item The $\epsilon$-packing number on $(\threeqset, d)$, denoted by $\packing(\epsilon, \threeqset, d)$, is the cardinality of the largest $\epsilon$-separated subset $\threeqset'$ of $\threeqset$.
        \end{itemize}
        Intuitively, the $\epsilon$-packing number is the maximal number of balls of radius $\epsilon/2$ that can fit into $\threeqset$ without intersecting.
    \end{definition}
\end{tcolorbox}

\begin{tcolorbox}
	\begin{definition}[Covered set and cover number]
		Let $(\threeqset, d)$ be a metric space. 
        \begin{itemize}
            \item For any $\epsilon> 0$, a subset $\threeqset'$ of $\threeqset$ is said to be an $\epsilon$-cover of $\threeqset$ if for all $\threequery \in \threeqset$, there exists $\threequery' \in \threeqset'$ with $d(\threequery, \threequery') \leq \epsilon$.
            \item The $\epsilon$-covering number on $(\threeqset, d)$, denoted by $\covering(\epsilon, \threeqset, d)$, is the cardinality of the smallest $\epsilon$-cover of $\threeqset$.
        \end{itemize}
        Intuitively, the $\epsilon$-covering number is the minimal number of balls of radius $\epsilon$ than can fill $\threeqset$, with possible overlaps.
	\end{definition}
\end{tcolorbox}

    One can easily check that for all $\epsilon>0$
    \begin{equation}
		\label{eqn__pack_cover_pack}
        \packing(2\epsilon, \threeqset, d) \leq \covering(\epsilon, \threeqset, d) \leq \packing(\epsilon, \threeqset, d)
    \end{equation}



Moreover, in the case of metric function spaces, packing numbers can be related to the pseudo-dimension of the packed space through

\begin{tcolorbox}
    \begin{theorem}[\cite{pollard1984_convergence_stoch_proc} and \cite{haussler1995spherepacking}]
        \label{thm_pack}
        For any set $\mathcal{X}$, any probability distribution $\mu$ on $\mathcal{X}$, any
        set $\threeqset \subseteq [0,\bound]^{\mathcal{X}}$ of $\mu$-measurable positive functions on $\mathcal{X}$ bounded by some real $\bound$, and any $\epsilon\in]0,\bound]$, one has
        \begin{align*}
            \packing(\epsilon, \threeqset, \dlone{\mu}{}{}) 
			\leq \min \left\{
			2\left(\frac{2eM}{\epsilon}\log\frac{2e\bound}{\epsilon}\right)^{d'_{\threeqset}},\ 
			e(d'_{\threeqset}+1) \left(\frac{2e \bound}{\epsilon}\right)^{d'_{\threeqset}}\right\}
        \end{align*}
        where $d'_{\threeqset} =\operatorname{pdim}\threeqset$ is the pseudo-dimension of $\threeqset$.
    \end{theorem}
\end{tcolorbox}
Note that the second bound is better than the first one when $\epsilon$ is sufficiently small compared to $d'_{\threeqset}$ and vice versa.

We introduce the following conjecture, as an attempt to extend Lemma 13. from \cite{haussler1992decisiontheoricgeneralizationofPACmodel} to a non-i.i.d. framework. We discuss in the draft of proof an inequality that would lead to the result. In addition, we discuss how the method used in i.i.d. case can not be applied easily to ours.


\begin{tcolorbox}
	\begin{conjecture}
		\label{lem_infi_union_bound}
		Let $\epsilon \in ]0,1]$, $m\in \NN$, and  $\mathcal{S}_1, \mathcal{S}_2 \overset{i.i.d.}{\sim} \PP{}{}$, two sequences of size $m$ independently sampled from the same distribution supported on $\mathcal{X}^m$. Then
		
		\begin{gather*}
			\PP{}{\exists \threequery \in \threeqset,\ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} > \epsilon} \\\leq \\
			\sup_{\mathcal{S} \in \mathcal{X}^{2m} } \covering(\epsilon/4, \threeqset, \dlone{\empdistr{\mathcal{S}}{}}{}{}) \sup_{\threequery \in \threeqset} \PP{}{\dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} > \epsilon/4 }
		\end{gather*}
	\end{conjecture}
\end{tcolorbox}


\begin{proof}[Draft of Proof]
	Let $\mathcal{S}_1$ and $\mathcal{S}_2$ sampled such that $\exists \threequery \in \threeqset,\ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} > \epsilon$. 
	
	We denote their concatenation by $\mathcal{S} := \mathcal{S}_1 \uplus \mathcal{S}_2 \in \mathcal{X}^{2m}$ which is of size $2m$. Let then be taken $\threeqset'_{\mathcal{S}}$, a minimal $\epsilon/4$-cover of $\threeqset$ for the $\dlone{\empdistr{\mathcal{S}}{}}{}{}$ distance, then $|\threeqset'_{\mathcal{S}}| = \covering(\epsilon/4, \threeqset, \dlone{\empdistr{\mathcal{S}}{}}{}{})$. We thus know there exists $\threequery' \in \threeqset'_{\mathcal{S}}$ such that $\dlone{\empdistr{\mathcal{S}}{}}{\threequery}{\threequery'} \leq \epsilon/4$.


	Then triangular inequalities yields that
	\begin{align*}
		\dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} &\leq \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery'}}{\empdistr{\mathcal{S}_2}{\threequery'}} + \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_1}{\threequery'}} + \dnude{\nu}{\empdistr{\mathcal{S}_2}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery'}}  \\
		&\leq \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery'}}{\empdistr{\mathcal{S}_2}{\threequery'}}  + \empdistr{\mathcal{S}_1}{\lvert \threequery-\threequery'\rvert} + \empdistr{\mathcal{S}_2}{\lvert\threequery-\threequery'\rvert}\\
		&\leq \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery'}}{\empdistr{\mathcal{S}_2}{\threequery'}} + 2 \dlone{\empdistr{\mathcal{S}}{}}{\threequery}{\threequery'}.
	\end{align*}
	Because of the $\epsilon/4$-cover, it implies 
	\begin{equation*}
		\dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery'}}{\empdistr{\mathcal{S}_2}{\threequery'}} \geq \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} - 2  \dlone{\empdistr{\mathcal{S}}{}}{\threequery}{\threequery'} > \epsilon/2
	\end{equation*}
	and therefore
	\begin{equation*}
		\PP{}{\exists \threequery \in \threeqset,\ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} > \epsilon} \leq \PP{}{\exists \threequery \in \threeqset'_{\mathcal{S}},\ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} > \epsilon/4}
	\end{equation*}
	By the law of total expectation, we obtain
	\begin{align*}
		\PP{}{\exists \threequery \in \threeqset'_{\mathcal{S}},\ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} > \epsilon/4}
		&=\EE{}{ \1 \{\exists \threequery \in \threeqset'_{\mathcal{S}},\ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} > \epsilon/4 \} }\\
		&=\EE{}{ \PP{}{\exists \threequery \in \threeqset'_{\mathcal{S}},\ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} > \epsilon/4 \mid \threeqset'_{\mathcal{S}}}  }\\
		&=\EE{}{ \PP{}{\bigcup_{\threequery \in \threeqset'_{\mathcal{S}}} \{ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} > \epsilon/4 \mid \threeqset'_{\mathcal{S}}\} }  }\\		
        &\overset{?}{\leq} \sup_{\threeqset'_{\mathcal{S}}} |\threeqset'_{\mathcal{S}}| \sup_{\threequery \in \threeqset} \PP{}{\dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} > \epsilon/4 }\\
		&= \sup_{\mathcal{S} \in \mathcal{X}^{2m} } N(\epsilon/4, \threeqset, \dlone{\empdistr{\mathcal{S}}{}}{}{} ) \sup_{\threequery \in \threeqset} \PP{}{\dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} > \epsilon/4 }
	\end{align*}
$\overset{?}{\leq}$ indicates this inequality is still to be proven. It consists of bounding the probability of a union of event over a random set. Since we can bound uniformly both, event probability, and set cardinality, it could seem intuitive this random union bound hold. However, the conditioning by $\threeqset'_{\mathcal{S}}$ makes this bound non trivial and likely requires further assumptions.


Though we make use of the symmetry between $\mathcal{S}_1$ and $\mathcal{S}_2$, we can't leverage it as much as the i.i.d. framework does. Such as in \cite{haussler1992decisiontheoricgeneralizationofPACmodel}, methods from PAC learning theory often leverage the fact that swapping samples from $\mathcal{S}_1$ and $\mathcal{S}_2$ preserves the i.i.d. sampling, which is not the case in general for a DPP sampling. For instance, if $\mathcal{X} = \{0,1\}$, then a $2$-DPP will return almost surely $\mathcal{S}_1 =\mathcal{S}_2 = \{0,1\}$. But swapping $0$ with $1$ leads to $\mathcal{S}_1 =\{0,0\}$ and $\mathcal{S}_2 =\{1,1\}$. This event has zero probability for a $2$-DPP sampling, so the sampling has not been preserved by the swapping.

\end{proof}




Admitting \cref{lem_infi_union_bound}, we are able to prove \cref{thm_infinite_union_bound}. 
\begin{proof}
	Let $\epsilon>0$ and take $m \in \NN$ such that $\boundrate(\epsilon/2, m) \leq 1/2$. Combining \cref{lem_symm} and \cref{lem_infi_union_bound} gives
	\begin{gather*}
		\PP{}{ \exists \threequery \in \threeqset,\ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\meanempdistr{\threequery}} > \epsilon} 
		\leq 2\PP{}{\exists \threequery \in \threeqset,\ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} > \epsilon/2 }\\
		\leq 2 \sup_{\mathcal{S} \in \mathcal{X}^{2m} } \covering(\epsilon/8, \threeqset, \dlone{\empdistr{\mathcal{S}}{}}{}{}) \sup_{\threequery \in \threeqset} \PP{}{\dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} > \epsilon/8 }
	\end{gather*}

In order to bound the last term, first consider applying the union bound
	\begin{align*}
		\PP{}{\dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}} > \epsilon/8 } 
		&\leq \PP{}{\dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\meanempdistr{\threequery}} + \dnude{\nu}{\meanempdistr{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}}> \epsilon/8 }\\
		&\leq \PP{}{\dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\meanempdistr{\threequery}} > \epsilon/16 \vee  \dnude{\nu}{\meanempdistr{\threequery}}{\empdistr{\mathcal{S}_2}{\threequery}}> \epsilon/16 }\\
		&\leq 2\PP{}{\dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\meanempdistr{\threequery}} > \epsilon/16 }\\
		&\leq 2 \boundrate(\threeqset, \epsilon/16, m)
	\end{align*}
Second, we know from \cref{eqn__pack_cover_pack} and \cref{thm_pack}
\begin{align*}
	\covering(\epsilon/8, \threeqset, \dlone{\empdistr{\mathcal{S}}{}}{}{})
	\leq \packing(\epsilon/8, \threeqset, \dlone{\empdistr{\mathcal{S}}{}}{}{})
	&\leq 2\left(\frac{16eM}{\epsilon}\log\frac{16e\bound}{\epsilon}\right)^{d'_{\threeqset}}
	&\leq  2\left(\frac{8e\bound}{\epsilon}\right)^{2d'_{\threeqset}}
\end{align*}
where we used the fact that $a \log a < (a/2)^2$ whenever $a \geq 5$, which is the case for $\frac{8eM}{\epsilon} \geq 5$ since $\epsilon \in ]0,\bound]$.

The two precedent bounds do neither depend on $\mathcal{S}$ nor $f$, and therefore 
\begin{align*}
	\PP{}{ \exists \threequery \in \threeqset,\ \dnude{\nu}{\empdistr{\mathcal{S}_1}{\threequery}}{\meanempdistr{\threequery}} > \epsilon} 
	&\leq 8\left(\frac{8e\bound}{\epsilon}\right)^{2d'_{\threeqset}}\boundrate(\epsilon/16, m)\\
\end{align*}
which is the desired result.\end{proof}


\chapter{Conclusion and perspectives}
\label{sec__discussion_perspectives}

In this report, we attempted to provide improved sample complexity results on coresets by the use of a well-chosen DPP. We started with an inventory on coreset literature, then we brought to light determinantal point processes as an improvement way. After qualitatively justifying the use of DPPs over existing sampling methods, we established quantitative results.

We show that under the assumption of \cref{lem_infi_union_bound}, still to be proven, a specific projective DPP based on orthogonal polynomials yields an improved sample complexity. Basically, previous state-of-the-art sample complexity is improved by a $(1+1/d)$-root. Also, it does not require to compute bounds on sensitivity, which can be a huge drawback of current methods depending on the problem. However, this result comes with some costs. 

First, it requires a supplementary regularity condition, namely, controlled Lipschitz constant. This requirement comes from the nature of the method used in \cite{bardenet2020mcdpp} to build the OPE kernel, which relies on a detour through continuous DPPs needed to apply continuous arguments. At the moment this report is redacted, it is not known if that detour can be shortcut in the discrete case. One idea could be to build a DPP kernel directly from a discrete OPE, i.e. to build an orthogonal polynomials family with respect to the empirical measure. When the data size is sufficiently large, one could recover the same behaviour.

Second, we saw in \cref{sec__sampling_DPP} that sampling from DPP comes with additional computational cost. In our case, sampling from the OPE kernel can be achieved in $\OO(nm^2)$, compared to $\OO(nm)$ in the i.i.d. case. One could rely on approximate DPP sampling, though it is not clear how sensible is our analysis to the exactness of the sampling.

Our analysis should rely on current results on concentration inequalities for DPPs, known achievable variance rates, generalization bound in the non-i.i.d. framework, and known DPP sampling methods. We made it sufficiently flexible to adapt to any potential improvements of these. We hope it provided a good insight on the potential improvements in machine learning applications one should expect from negatively correlated sampling, and especially DPPs. This work was intended to be the most complete on the theoretical level. The next step would now be to confirm it empirically via numerical experiments.


Finally, I can't close this report without thanking the team SigMA from CRIStAL, for its good spirit and understanding that has greatly contributed to dilute the worries of a broken arm while biking. In particular, I acknowledge R\'emi Bardenet motivation, openness, relevance and the energy he invests in involving everyone into its scientific adventures.





