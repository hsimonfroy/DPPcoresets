@article{tremblay2018dppcoreset,
  doi = {10.48550/ARXIV.1803.08700},
  url = {https://arxiv.org/abs/1803.08700},
  author = {Tremblay, Nicolas and Barthelmé, Simon and Amblard, Pierre-Olivier},
  keywords = {Machine Learning (stat.ML), Data Structures and Algorithms (cs.DS), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Determinantal Point Processes for Coresets},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{bachem2017coresetML,
  doi = {10.48550/ARXIV.1703.06476},
  url = {https://arxiv.org/abs/1703.06476},
  author = {Bachem, Olivier and Lucic, Mario and Krause, Andreas},
  keywords = {Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Practical Coreset Constructions for Machine Learning},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{zhang2017dppminibatch,
  doi = {10.48550/ARXIV.1705.00607},
  url = {https://arxiv.org/abs/1705.00607},
  author = {Zhang, Cheng and Kjellstrom, Hedvig and Mandt, Stephan},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Determinantal Point Processes for Mini-Batch Diversification},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@article{bardenet2020mcdpp,
  TITLE = {{Monte Carlo with Determinantal Point Processes}},
  AUTHOR = {Bardenet, R{\'e}mi and Hardy, Adrien},
  URL = {https://hal.archives-ouvertes.fr/hal-01311263},
  JOURNAL = {{Annals of Applied Probability}},
  PUBLISHER = {{Institute of Mathematical Statistics (IMS)}},
  YEAR = {2020},
  PDF = {https://hal.archives-ouvertes.fr/hal-01311263/file/1605.00361v1.pdf},
  HAL_ID = {hal-01311263},
  HAL_VERSION = {v1},
}



@misc{bardenet2021sgddpp,
  doi = {10.48550/ARXIV.2112.06007},
  url = {https://arxiv.org/abs/2112.06007},
  author = {Bardenet, R{\'e}mi and Ghosh, Subhro and Lin, Meixia},
  keywords = {Machine Learning (stat.ML), Disordered Systems and Neural Networks (cond-mat.dis-nn), Machine Learning (cs.LG), Optimization and Control (math.OC), Probability (math.PR), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences, FOS: Mathematics, FOS: Mathematics},
  title = {Determinantal point processes based on orthogonal polynomials for sampling minibatches in SGD},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Zero v1.0 Universal}
}



@misc{copenhaver2013diagramvectors,
  doi = {10.48550/ARXIV.1303.1159},
  
  url = {https://arxiv.org/abs/1303.1159},
  
  author = {Copenhaver, Martin S. and Kim, Yeon Hyang and Logan, Cortney and Mayfield, Kyanne and Narayan, Sivaram K. and Petro, Matthew J. and Sheperd, Jonathan},
  
  keywords = {Functional Analysis (math.FA), FOS: Mathematics, FOS: Mathematics, 42C15, 05B20, 15A03},
  
  title = {Diagram vectors and Tight Frame Scaling in Finite Dimensions},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{braverman2016coresetsota,
  doi = {10.48550/ARXIV.1612.00889},
  
  url = {https://arxiv.org/abs/1612.00889},
  
  author = {Braverman, Vladimir and Feldman, Dan and Lang, Harry and Statman, Adiel and Zhou, Samson},
  
  keywords = {Data Structures and Algorithms (cs.DS), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {New Frameworks for Offline and Streaming Coreset Constructions},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {Creative Commons Attribution 4.0 International}
}



@misc{pemantle2011rayleighconcentration,
  doi = {10.48550/ARXIV.1108.0687},
  
  url = {https://arxiv.org/abs/1108.0687},
  
  author = {Pemantle, Robin and Peres, Yuval},
  
  keywords = {Probability (math.PR), FOS: Mathematics, FOS: Mathematics, 60G55, 60E15},
  
  title = {Concentration of Lipschitz functionals of determinantal and other strong Rayleigh measures},
  
  publisher = {arXiv},
  
  year = {2011},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@book {gautschi2004ope,
	title = {Orthogonal Polynomials: Computation and Approximation},
	booktitle = {Orthogonal Polynomials: Computation and Approximation},
	eprint = {https://www.cs.purdue.edu/homes/wxg/OPmatlab.pdf},
	institution = {Oxford},
	publisher = {Clarendon Press},
	series = {Numerical Mathematics and Scientific Computation},
	year = {2004},
	isbn = {0198506724},
	author = {Gautschi , Walter}
}



@misc{breuer2013nevai,
  doi = {10.48550/ARXIV.1301.2061},
  
  url = {https://arxiv.org/abs/1301.2061},
  
  author = {Breuer, Jonathan and Duits, Maurice},
  
  keywords = {Probability (math.PR), Mathematical Physics (math-ph), FOS: Mathematics, FOS: Mathematics, FOS: Physical sciences, FOS: Physical sciences},
  
  title = {The Nevai condition and a local law of large numbers for orthogonal polynomial ensembles},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@article{haussler1995spherepacking,
title = {Sphere packing numbers for subsets of the Boolean n-cube with bounded Vapnik-Chervonenkis dimension},
journal = {Journal of Combinatorial Theory, Series A},
volume = {69},
number = {2},
pages = {217-232},
year = {1995},
issn = {0097-3165},
doi = {https://doi.org/10.1016/0097-3165(95)90052-7},
url = {https://www.sciencedirect.com/science/article/pii/0097316595900527},
author = {David Haussler},
abstract = {Let V ⊆ {0, 1}n have Vapnik-Chervonenkis dimension d. Let M(k/n, V) denote the cardinality of the largest W ⊆ V such that any two distinct vectors in W differ on at least k indices. We show that M(k/n, V) ≤ (cn/(k + d))d for some constant c. This improves on the previous best result of ((cnk)log(nk))d. This new result has applications in the theory of empirical processes.}
}




@article{haussler1992decisiontheoricgeneralizationofPACmodel,
title = {Decision theoretic generalizations of the PAC model for neural net and other learning applications},
journal = {Information and Computation},
volume = {100},
number = {1},
pages = {78-150},
year = {1992},
issn = {0890-5401},
doi = {https://doi.org/10.1016/0890-5401(92)90010-D},
url = {https://www.sciencedirect.com/science/article/pii/089054019290010D},
author = {David Haussler},
abstract = {We describe a generalization of the PAC learning model that is based on statistical decision theory. In this model the learner receives randomly drawn examples, each example consisting of an instance x ∈ X and an outcome y ∈ Y, and tries to find a decision rule h: X → A, where h ∈ H, that specifies the appropriate action a ∈ A to take for each instance x in order to minimize the expectation of a loss l(y, a). Here X, Y, and A are arbitrary sets, l is a real-valued function, and examples are generated according to an arbitrary joint distribution on X × Y. Special cases include the problem of learning a function from X into Y, the problem of learning the conditional probability distribution on Y given X (regression), and the problem of learning a distribution on X (density estimation). We give theorems on the uniform convergence of empirical loss estimates to true expected loss rates for certain decision rule spaces H, and show how this implies learnability with bounded sample size, disregarding computational complexity. As an application, we give distribution-independent upper bounds on the sample size needed for learning with feedforward neural networks. Our theorems use a generalized notion of VC dimension that applies to classes of real-valued functions, adapted from Vapnik and Pollard's work, and a notion of capacity and metric dimension for classes of functions that map into a bounded metric space.}
}


