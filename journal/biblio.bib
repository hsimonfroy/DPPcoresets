@article{tremblay2018dppcoreset,
  doi = {10.48550/ARXIV.1803.08700},
  url = {https://arxiv.org/abs/1803.08700},
  author = {Tremblay, Nicolas and Barthelmé, Simon and Amblard, Pierre-Olivier},
  keywords = {Machine Learning (stat.ML), Data Structures and Algorithms (cs.DS), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Determinantal Point Processes for Coresets},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{bachem2017coresetML,
  doi = {10.48550/ARXIV.1703.06476},
  url = {https://arxiv.org/abs/1703.06476},
  author = {Bachem, Olivier and Lucic, Mario and Krause, Andreas},
  keywords = {Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Practical Coreset Constructions for Machine Learning},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{zhang2017dppminibatch,
  doi = {10.48550/ARXIV.1705.00607},
  url = {https://arxiv.org/abs/1705.00607},
  author = {Zhang, Cheng and Kjellstrom, Hedvig and Mandt, Stephan},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Determinantal Point Processes for Mini-Batch Diversification},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@article{bardenet2020mcdpp,
  TITLE = {{Monte Carlo with Determinantal Point Processes}},
  AUTHOR = {Bardenet, R{\'e}mi and Hardy, Adrien},
  URL = {https://hal.archives-ouvertes.fr/hal-01311263},
  JOURNAL = {{Annals of Applied Probability}},
  PUBLISHER = {{Institute of Mathematical Statistics (IMS)}},
  YEAR = {2020},
  PDF = {https://hal.archives-ouvertes.fr/hal-01311263/file/1605.00361v1.pdf},
  HAL_ID = {hal-01311263},
  HAL_VERSION = {v1},
}



@misc{bardenet2021sgddpp,
  doi = {10.48550/ARXIV.2112.06007},
  url = {https://arxiv.org/abs/2112.06007},
  author = {Bardenet, R{\'e}mi and Ghosh, Subhro and Lin, Meixia},
  keywords = {Machine Learning (stat.ML), Disordered Systems and Neural Networks (cond-mat.dis-nn), Machine Learning (cs.LG), Optimization and Control (math.OC), Probability (math.PR), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences, FOS: Mathematics, FOS: Mathematics},
  title = {Determinantal point processes based on orthogonal polynomials for sampling minibatches in SGD},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Zero v1.0 Universal}
}



@misc{copenhaver2013diagramvectors,
  doi = {10.48550/ARXIV.1303.1159},
  
  url = {https://arxiv.org/abs/1303.1159},
  
  author = {Copenhaver, Martin S. and Kim, Yeon Hyang and Logan, Cortney and Mayfield, Kyanne and Narayan, Sivaram K. and Petro, Matthew J. and Sheperd, Jonathan},
  
  keywords = {Functional Analysis (math.FA), FOS: Mathematics, FOS: Mathematics, 42C15, 05B20, 15A03},
  
  title = {Diagram vectors and Tight Frame Scaling in Finite Dimensions},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{braverman2016coresetsota,
  doi = {10.48550/ARXIV.1612.00889},
  
  url = {https://arxiv.org/abs/1612.00889},
  
  author = {Braverman, Vladimir and Feldman, Dan and Lang, Harry and Statman, Adiel and Zhou, Samson},
  
  keywords = {Data Structures and Algorithms (cs.DS), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {New Frameworks for Offline and Streaming Coreset Constructions},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {Creative Commons Attribution 4.0 International}
}



@misc{pemantle2011rayleighconcentration,
  doi = {10.48550/ARXIV.1108.0687},
  
  url = {https://arxiv.org/abs/1108.0687},
  
  author = {Pemantle, Robin and Peres, Yuval},
  
  keywords = {Probability (math.PR), FOS: Mathematics, FOS: Mathematics, 60G55, 60E15},
  
  title = {Concentration of Lipschitz functionals of determinantal and other strong Rayleigh measures},
  
  publisher = {arXiv},
  
  year = {2011},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@book {gautschi2004ope,
	title = {Orthogonal Polynomials: Computation and Approximation},
	booktitle = {Orthogonal Polynomials: Computation and Approximation},
	eprint = {https://www.cs.purdue.edu/homes/wxg/OPmatlab.pdf},
	institution = {Oxford},
	publisher = {Clarendon Press},
	series = {Numerical Mathematics and Scientific Computation},
	year = {2004},
	isbn = {0198506724},
	author = {Gautschi , Walter}
}



@misc{breuer2013nevai,
  doi = {10.48550/ARXIV.1301.2061},
  
  url = {https://arxiv.org/abs/1301.2061},
  
  author = {Breuer, Jonathan and Duits, Maurice},
  
  keywords = {Probability (math.PR), Mathematical Physics (math-ph), FOS: Mathematics, FOS: Mathematics, FOS: Physical sciences, FOS: Physical sciences},
  
  title = {The Nevai condition and a local law of large numbers for orthogonal polynomial ensembles},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@article{haussler1995spherepacking,
title = {Sphere packing numbers for subsets of the Boolean n-cube with bounded Vapnik-Chervonenkis dimension},
journal = {Journal of Combinatorial Theory, Series A},
volume = {69},
number = {2},
pages = {217-232},
year = {1995},
issn = {0097-3165},
doi = {https://doi.org/10.1016/0097-3165(95)90052-7},
url = {https://www.sciencedirect.com/science/article/pii/0097316595900527},
author = {David Haussler},
abstract = {Let V ⊆ {0, 1}n have Vapnik-Chervonenkis dimension d. Let M(k/n, V) denote the cardinality of the largest W ⊆ V such that any two distinct vectors in W differ on at least k indices. We show that M(k/n, V) ≤ (cn/(k + d))d for some constant c. This improves on the previous best result of ((cnk)log(nk))d. This new result has applications in the theory of empirical processes.}
}




@article{haussler1992decisiontheoricgeneralizationofPACmodel,
title = {Decision theoretic generalizations of the PAC model for neural net and other learning applications},
journal = {Information and Computation},
volume = {100},
number = {1},
pages = {78-150},
year = {1992},
issn = {0890-5401},
doi = {https://doi.org/10.1016/0890-5401(92)90010-D},
url = {https://www.sciencedirect.com/science/article/pii/089054019290010D},
author = {David Haussler},
abstract = {We describe a generalization of the PAC learning model that is based on statistical decision theory. In this model the learner receives randomly drawn examples, each example consisting of an instance x ∈ X and an outcome y ∈ Y, and tries to find a decision rule h: X → A, where h ∈ H, that specifies the appropriate action a ∈ A to take for each instance x in order to minimize the expectation of a loss l(y, a). Here X, Y, and A are arbitrary sets, l is a real-valued function, and examples are generated according to an arbitrary joint distribution on X × Y. Special cases include the problem of learning a function from X into Y, the problem of learning the conditional probability distribution on Y given X (regression), and the problem of learning a distribution on X (density estimation). We give theorems on the uniform convergence of empirical loss estimates to true expected loss rates for certain decision rule spaces H, and show how this implies learnability with bounded sample size, disregarding computational complexity. As an application, we give distribution-independent upper bounds on the sample size needed for learning with feedforward neural networks. Our theorems use a generalized notion of VC dimension that applies to classes of real-valued functions, adapted from Vapnik and Pollard's work, and a notion of capacity and metric dimension for classes of functions that map into a bounded metric space.}
}



@article{valiant1984learnable,
author = {Valiant, L. G.},
title = {A Theory of the Learnable},
year = {1984},
issue_date = {Nov. 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/1968.1972},
doi = {10.1145/1968.1972},
journal = {Commun. ACM},
month = {nov},
pages = {1134–1142},
numpages = {9},
keywords = {inductive inference, propositional expressions, probabilistic models of learning}
}



@article{macchi1975dpp, title={The coincidence approach to stochastic point processes}, volume={7}, DOI={10.2307/1425855}, number={1}, journal={Advances in Applied Probability}, publisher={Cambridge University Press}, author={Macchi, Odile}, year={1975}, pages={83–122}}



@article{hough2006_hkpv,
	doi = {10.1214/154957806000000078},
  
	url = {https://doi.org/10.1214%2F154957806000000078},
  
	year = 2006,
	month = {jan},
  
	publisher = {Institute of Mathematical Statistics},
  
	volume = {3},
  
	number = {none},
  
	author = {J. Ben Hough and Manjunath Krishnapur and Yuval Peres and B{\'{a}
}lint Vir{\'{a}}g},
  
	title = {Determinantal Processes and Independence},
  
	journal = {Probability Surveys}
}


@article{poulson2020_high_performance_DPP_sampling,
	doi = {10.1098/rsta.2019.0059},
  
	url = {https://doi.org/10.1098%2Frsta.2019.0059},
  
	year = 2020,
	month = {jan},
  
	publisher = {The Royal Society},
  
	volume = {378},
  
	number = {2166},
  
	pages = {20190059},
  
	author = {Jack Poulson},
  
	title = {High-performance sampling of generic determinantal point processes},
  
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences}
}



@article{launay2020_dpp_sampling_without_eigendecomposition,
	doi = {10.1017/jpr.2020.56},
  
	url = {https://doi.org/10.1017%2Fjpr.2020.56},
  
	year = 2020,
	month = {nov},
  
	publisher = {Cambridge University Press ({CUP})},
  
	volume = {57},
  
	number = {4},
  
	pages = {1198--1221},
  
	author = {Claire Launay and Bruno Galerne and Agn{\`{e}
}s Desolneux},
  
	title = {Exact sampling of determinantal point processes without eigendecomposition},
  
	journal = {Journal of Applied Probability}
}




@InProceedings{gillenwater2019_treebased_fast_dpp_sampling,
  title = 	 {A Tree-Based Method for Fast Repeated Sampling of Determinantal Point Processes},
  author =       {Gillenwater, Jennifer and Kulesza, Alex and Mariet, Zelda and Vassilvtiskii, Sergei},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2260--2268},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/gillenwater19a/gillenwater19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/gillenwater19a.html},
  abstract = 	 {It is often desirable in recommender systems and other information retrieval applications to provide diverse results, and determinantal point processes (DPPs) have become a popular way to capture the trade-off between the quality of individual results and the diversity of the overall set. However, sampling from a DPP is inherently expensive: if the underlying collection contains N items, then generating each DPP sample requires time linear in N following a one-time preprocessing phase. Additionally, results often need to be personalized to a user, but standard approaches to personalization invalidate the preprocessing, making personalized samples especially expensive. In this work we address both of these shortcomings. First, we propose a new algorithm for generating DPP samples in time logarithmic in N, following a slightly more expensive preprocessing phase. We then extend the algorithm to support arbitrary query-time feature weights, allowing us to generate samples customized to individual users while still retaining logarithmic runtime; experiments show our approach runs over 300 times faster than traditional DPP sampling on collections of 100,000 items for samples of size 10.}
}




@article{kulesza2012_dpp_for_ml,
url = {http://dx.doi.org/10.1561/2200000044},
year = {2012},
volume = {5},
journal = {Foundations and Trends® in Machine Learning},
title = {Determinantal Point Processes for Machine Learning},
doi = {10.1561/2200000044},
issn = {1935-8237},
number = {2–3},
pages = {123-286},
author = {Alex Kulesza and Ben Taskar}
}





@InProceedings{gautier2017_zonotope_for_dpp_sampling,
  title = 	 {Zonotope Hit-and-run for Efficient Sampling from Projection {DPP}s},
  author =       {Guillaume Gautier and R{\'e}mi Bardenet and Michal Valko},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1223--1232},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/gautier17a/gautier17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/gautier17a.html},
  abstract = 	 {Determinantal point processes (DPPs) are distributions over sets of items that model diversity using kernels. Their applications in machine learning include summary extraction and recommendation systems. Yet, the cost of sampling from a DPP is prohibitive in large-scale applications, which has triggered an effort towards efficient approximate samplers. We build a novel MCMC sampler that combines ideas from combinatorial geometry, linear programming, and Monte Carlo methods to sample from DPPs with a fixed sample cardinality, also called projection DPPs. Our sampler leverages the ability of the hit-and-run MCMC kernel to efficiently move across convex bodies. Previous theoretical results yield a fast mixing time of our chain when targeting a distribution that is close to a projection DPP, but not a DPP in general. Our empirical results demonstrate that this extends to sampling projection DPPs, i.e., our sampler is more sample-efficient than previous approaches which in turn translates to faster convergence when dealing with costly-to-evaluate functions, such as summary extraction in our experiments.}
}



@book{pollard1984_convergence_stoch_proc,
  title={Convergence of stochastic processes},
  author={Pollard, David},
  year={1984},
  publisher={Springer Science \& Business Media}
}




@inproceedings{langberg2010_universal_approximator,
author = {Langberg, Michael and Schulman, Leonard},
year = {2010},
month = {01},
pages = {},
title = {Universal epsilon-approximators for integrals}
}



@misc{lucic2016_lineartime_detection_via_sensitivity,
  doi = {10.48550/ARXIV.1605.00519},
  url = {https://arxiv.org/abs/1605.00519},
  author = {Lucic, Mario and Bachem, Olivier and Krause, Andreas},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Linear-time Outlier Detection via Sensitivity},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@article{li2001_improved_bound_sample_complexity,
title = {Improved Bounds on the Sample Complexity of Learning},
journal = {Journal of Computer and System Sciences},
volume = {62},
number = {3},
pages = {516-527},
year = {2001},
issn = {0022-0000},
doi = {https://doi.org/10.1006/jcss.2000.1741},
url = {https://www.sciencedirect.com/science/article/pii/S0022000000917410},
author = {Yi Li and Philip M. Long and Aravind Srinivasan},
keywords = {sample complexity, machine learning, empirical process theory, PAC learning, agnostic learning},
abstract = {We present a new general upper bound on the number of examples required to estimate all of the expectations of a set of random variables uniformly well. The quality of the estimates is measured using a variant of the relative error proposed by Haussler and Pollard. We also show that our bound is within a constant factor of the best possible. Our upper bound implies improved bounds on the sample complexity of learning according to Haussler's decision theoretic model.}
}




@InProceedings{gao2016_learnability_beta_mixing,
  title = 	 {Learnability of Non-I.I.D.},
  author = 	 {Gao, Wei and Niu, Xin-Yi and Zhou, Zhi-Hua},
  booktitle = 	 {Proceedings of The 8th Asian Conference on Machine Learning},
  pages = 	 {158--173},
  year = 	 {2016},
  editor = 	 {Durrant, Robert J. and Kim, Kee-Eung},
  volume = 	 {63},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {The University of Waikato, Hamilton, New Zealand},
  month = 	 {16--18 Nov},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v63/Gao09.pdf},
  url = 	 {https://proceedings.mlr.press/v63/Gao09.html},
  abstract = 	 {Learnability has always been one of the most central problems in learning theory. Most previous studies on this issue were based on the assumption that the samples are drawn independently and identically according to an underlying (unknown) distribution. The i.i.d. assumption, however, does not hold in many real applications. In this paper, we study the learnability of problems where the samples are drawn from empirical process of stationary β-mixing sequence, which has been a widely-used assumption implying a dependence weaken over time in training samples. By utilizing the independent blocks technique, we provide a sufficient and necessary condition for learnability, that is, average stability is equivalent to learnability with AERM (Asymptotic Empirical Risk Minimization) in the non-i.i.d. learning setting. In addition, we also discuss the generalization error when the test variable is dependent on the training sample.}
}
